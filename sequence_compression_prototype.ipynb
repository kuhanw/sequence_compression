{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import itertools\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createLSTMLayers(n_layers, n_cells, dropout):\n",
    "    cell_list = []\n",
    "    \n",
    "    for layer in range(n_layers):\n",
    "        cell = tf.contrib.rnn.LayerNormBasicLSTMCell(n_cells, activation=tf.nn.relu, layer_norm=False)\n",
    "        \n",
    "        cell_list.append(cell)\n",
    "        \n",
    "    return cell_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padSeq(seqs, pad_length, right_pad=True):\n",
    "    \n",
    "    '''\n",
    "    padSeq\n",
    "    \n",
    "    Pad sequences to be equal length.\n",
    "    \n",
    "    seqs: sequences to be padded\n",
    "    pad_length: length to pad sequences\n",
    "    left_pad: Pad at the end of the sequences, or reverse pad from front\n",
    "    '''\n",
    "    \n",
    "    padded_seqs = np.zeros((len(seqs), pad_length))\n",
    "    \n",
    "    for idx_row, row in enumerate(seqs):\n",
    "        for idx_col, col in enumerate(row):\n",
    "            if right_pad:\n",
    "                padded_seqs[idx_row, idx_col] = seqs[idx_row][idx_col]\n",
    "            else:\n",
    "                padded_seqs[idx_row, pad_length-idx_col-1] = seqs[idx_row][len(row)-idx_col-1]\n",
    "    return padded_seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Network parameters\n",
    "encoder_layers = 1\n",
    "encoder_cells = 5\n",
    "encoder_dropout = 1\n",
    "\n",
    "decoder_layers = 1\n",
    "decoder_cells = 5\n",
    "decoder_dropout = 1\n",
    "\n",
    "seq_length = 5\n",
    "n_features = 1\n",
    "\n",
    "latent_dimensions = 2\n",
    "\n",
    "#Training Parameters\n",
    "lr = 0.001\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.placeholder(name='input', shape=[None, seq_length, n_features], dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoder\n",
    "with tf.variable_scope('encoder', reuse=False):\n",
    "    \n",
    "    encoder_cell_fw = tf.contrib.rnn.MultiRNNCell(createLSTMLayers(encoder_layers, encoder_cells, encoder_dropout))\n",
    "    encoder_cell_bw = tf.contrib.rnn.MultiRNNCell(createLSTMLayers(encoder_layers, encoder_cells, encoder_dropout))\n",
    "    \n",
    "    (encoder_fw_outputs, encoder_bw_outputs), encoder_state_outputs = tf.nn.bidirectional_dynamic_rnn(\n",
    "                                        encoder_cell_fw,\n",
    "                                        encoder_cell_bw, \n",
    "                                        inputs=inputs,\n",
    "                                        dtype=tf.float32, time_major=False, swap_memory=True)\n",
    "    \n",
    "    encoder_final_state = tf.concat([state_tuple[0].h for state_tuple in encoder_state_outputs], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_vector_in = tf.contrib.layers.fully_connected(encoder_final_state, latent_dimensions, activation_fn=tf.nn.relu,\n",
    "                                                    normalizer_fn=tf.contrib.layers.batch_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_vector_out = tf.contrib.layers.fully_connected(latent_vector_in, decoder_cells, activation_fn=tf.nn.relu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_inputs = tf.zeros([tf.shape(inputs)[0], seq_length, n_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_initial_state = tuple([tf.nn.rnn_cell.LSTMStateTuple(c=latent_vector_out, h=latent_vector_out) for i in \n",
    "                              range(decoder_layers)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#decoder\n",
    "\n",
    "with tf.variable_scope('decoder', reuse=False):\n",
    "    \n",
    "    decoder_cell_fw = tf.contrib.rnn.MultiRNNCell(createLSTMLayers(decoder_layers, decoder_cells, decoder_dropout))\n",
    "    decoder_cell_bw = tf.contrib.rnn.MultiRNNCell(createLSTMLayers(decoder_layers, decoder_cells, decoder_dropout))\n",
    "    \n",
    "    (decoder_fw_outputs, decoder_bw_outputs), decoder_state_outputs = tf.nn.bidirectional_dynamic_rnn(\n",
    "                                        decoder_cell_fw,\n",
    "                                        decoder_cell_bw, \n",
    "                                        inputs=decoder_inputs,\n",
    "                                        initial_state_fw=decoder_initial_state,\n",
    "                                        initial_state_bw=decoder_initial_state,\n",
    "                                        dtype=tf.float32, time_major=False, swap_memory=True)\n",
    "    \n",
    "    decoder_outputs = tf.concat([decoder_fw_outputs, decoder_bw_outputs], axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_layer = tf.contrib.layers.fully_connected(decoder_outputs, n_features, activation_fn=tf.nn.relu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.reduce_sum(tf.square(inputs-output_layer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(learning_rate=lr).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generatePseudoSequences(seq_length, low, high, n_seqs, seed, variable_length=False):\n",
    "    \n",
    "    '''\n",
    "    generatePseudoSequences\n",
    "    \n",
    "    seq_length: Length of sequence to generate\n",
    "    low: lower bound of values\n",
    "    high: upper bound of values\n",
    "    n_seqs: number of sequences to generate\n",
    "    variable_length: Create sequence of variable length    \n",
    "    '''\n",
    "    random.seed(seed)\n",
    "    #Generate random sequences\n",
    "    random_seqs = []\n",
    "    for seq in range(n_seqs):\n",
    "        if variable_length: current_length = random.randint(1, seq_length)\n",
    "        else: current_length = seq_length\n",
    "        current_seq = [random.randint(low, high) for _ in range(current_length)]\n",
    "        \n",
    "        random_seqs.append(current_seq)\n",
    "        \n",
    "    #Generate repeating sequences\n",
    "    \n",
    "    repeat_seqs = []\n",
    "    for seq in range(n_seqs):\n",
    "        if variable_length: current_length = random.randint(1, seq_length)\n",
    "        else: current_length = seq_length\n",
    "        seq_value = random.randint(low, high)\n",
    "        current_seq = [seq_value for _ in range(current_length)]\n",
    "        \n",
    "        repeat_seqs.append(current_seq)\n",
    "        \n",
    "    return repeat_seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_seqs = pd.DataFrame(padSeq(generatePseudoSequences(5, 1, 10, 50000, 0, variable_length=False), 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_cols = df_seqs.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_seqs['label'] = df_seqs[seq_cols].apply(lambda x:'_'.join([str(i) for i in x]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.0_6.0_6.0_6.0_6.0         5142\n",
       "2.0_2.0_2.0_2.0_2.0         5091\n",
       "7.0_7.0_7.0_7.0_7.0         5034\n",
       "5.0_5.0_5.0_5.0_5.0         5018\n",
       "1.0_1.0_1.0_1.0_1.0         4978\n",
       "10.0_10.0_10.0_10.0_10.0    4962\n",
       "9.0_9.0_9.0_9.0_9.0         4953\n",
       "3.0_3.0_3.0_3.0_3.0         4948\n",
       "8.0_8.0_8.0_8.0_8.0         4940\n",
       "4.0_4.0_4.0_4.0_4.0         4934\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_seqs['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = model_selection.train_test_split(df_seqs, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training\n",
    "\n",
    "n_epochs = 50\n",
    "\n",
    "n_batches = df_train.shape[0]//batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 312\n"
     ]
    }
   ],
   "source": [
    "print (n_epochs, n_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 236.81265259,    0.        ],\n",
       "       [  36.28363419,    0.        ],\n",
       "       [ 385.15914917,    0.        ],\n",
       "       [ 433.4881897 ,    0.        ],\n",
       "       [ 481.52926636,    0.        ],\n",
       "       [  36.28363419,    0.        ],\n",
       "       [ 132.7492218 ,    0.        ],\n",
       "       [ 481.52926636,    0.        ],\n",
       "       [ 185.18772888,    0.        ],\n",
       "       [  36.28363419,    0.        ],\n",
       "       [ 385.15914917,    0.        ],\n",
       "       [ 481.52926636,    0.        ],\n",
       "       [ 385.15914917,    0.        ],\n",
       "       [ 336.45236206,    0.        ],\n",
       "       [ 433.4881897 ,    0.        ],\n",
       "       [ 132.7492218 ,    0.        ],\n",
       "       [ 287.13442993,    0.        ],\n",
       "       [ 336.45236206,    0.        ],\n",
       "       [ 132.7492218 ,    0.        ],\n",
       "       [ 336.45236206,    0.        ],\n",
       "       [  36.28363419,    0.        ],\n",
       "       [  36.28363419,    0.        ],\n",
       "       [ 385.15914917,    0.        ],\n",
       "       [ 132.7492218 ,    0.        ],\n",
       "       [  81.57950592,    0.        ],\n",
       "       [ 132.7492218 ,    0.        ],\n",
       "       [ 236.81265259,    0.        ],\n",
       "       [ 236.81265259,    0.        ],\n",
       "       [  81.57950592,    0.        ],\n",
       "       [ 236.81265259,    0.        ],\n",
       "       [ 132.7492218 ,    0.        ],\n",
       "       [ 481.52926636,    0.        ],\n",
       "       [  81.57950592,    0.        ],\n",
       "       [ 287.13442993,    0.        ],\n",
       "       [  36.28363419,    0.        ],\n",
       "       [ 132.7492218 ,    0.        ],\n",
       "       [ 481.52926636,    0.        ],\n",
       "       [ 132.7492218 ,    0.        ],\n",
       "       [ 481.52926636,    0.        ],\n",
       "       [ 287.13442993,    0.        ],\n",
       "       [ 287.13442993,    0.        ],\n",
       "       [ 336.45236206,    0.        ],\n",
       "       [ 481.52926636,    0.        ],\n",
       "       [ 132.7492218 ,    0.        ],\n",
       "       [  36.28363419,    0.        ],\n",
       "       [ 287.13442993,    0.        ],\n",
       "       [  81.57950592,    0.        ],\n",
       "       [ 433.4881897 ,    0.        ],\n",
       "       [ 433.4881897 ,    0.        ],\n",
       "       [ 336.45236206,    0.        ],\n",
       "       [ 185.18772888,    0.        ],\n",
       "       [ 236.81265259,    0.        ],\n",
       "       [ 185.18772888,    0.        ],\n",
       "       [ 132.7492218 ,    0.        ],\n",
       "       [ 481.52926636,    0.        ],\n",
       "       [  36.28363419,    0.        ],\n",
       "       [ 132.7492218 ,    0.        ],\n",
       "       [ 236.81265259,    0.        ],\n",
       "       [ 236.81265259,    0.        ],\n",
       "       [  36.28363419,    0.        ],\n",
       "       [ 185.18772888,    0.        ],\n",
       "       [ 433.4881897 ,    0.        ],\n",
       "       [ 433.4881897 ,    0.        ],\n",
       "       [ 481.52926636,    0.        ],\n",
       "       [  81.57950592,    0.        ],\n",
       "       [  36.28363419,    0.        ],\n",
       "       [ 385.15914917,    0.        ],\n",
       "       [ 185.18772888,    0.        ],\n",
       "       [  81.57950592,    0.        ],\n",
       "       [ 132.7492218 ,    0.        ],\n",
       "       [ 287.13442993,    0.        ],\n",
       "       [ 185.18772888,    0.        ],\n",
       "       [ 236.81265259,    0.        ],\n",
       "       [ 481.52926636,    0.        ],\n",
       "       [ 236.81265259,    0.        ],\n",
       "       [ 481.52926636,    0.        ],\n",
       "       [ 287.13442993,    0.        ],\n",
       "       [ 236.81265259,    0.        ],\n",
       "       [ 481.52926636,    0.        ],\n",
       "       [ 481.52926636,    0.        ],\n",
       "       [ 385.15914917,    0.        ],\n",
       "       [ 481.52926636,    0.        ],\n",
       "       [ 385.15914917,    0.        ],\n",
       "       [ 336.45236206,    0.        ],\n",
       "       [ 185.18772888,    0.        ],\n",
       "       [ 185.18772888,    0.        ],\n",
       "       [ 336.45236206,    0.        ],\n",
       "       [ 287.13442993,    0.        ],\n",
       "       [ 185.18772888,    0.        ],\n",
       "       [ 481.52926636,    0.        ],\n",
       "       [ 481.52926636,    0.        ],\n",
       "       [ 336.45236206,    0.        ],\n",
       "       [ 481.52926636,    0.        ],\n",
       "       [ 385.15914917,    0.        ],\n",
       "       [ 132.7492218 ,    0.        ],\n",
       "       [ 336.45236206,    0.        ],\n",
       "       [ 236.81265259,    0.        ],\n",
       "       [ 481.52926636,    0.        ],\n",
       "       [ 433.4881897 ,    0.        ],\n",
       "       [  81.57950592,    0.        ],\n",
       "       [ 287.13442993,    0.        ],\n",
       "       [ 385.15914917,    0.        ],\n",
       "       [ 236.81265259,    0.        ],\n",
       "       [ 236.81265259,    0.        ],\n",
       "       [  81.57950592,    0.        ],\n",
       "       [ 336.45236206,    0.        ],\n",
       "       [  36.28363419,    0.        ],\n",
       "       [ 185.18772888,    0.        ],\n",
       "       [  81.57950592,    0.        ],\n",
       "       [ 132.7492218 ,    0.        ],\n",
       "       [ 287.13442993,    0.        ],\n",
       "       [ 185.18772888,    0.        ],\n",
       "       [  36.28363419,    0.        ],\n",
       "       [ 132.7492218 ,    0.        ],\n",
       "       [  81.57950592,    0.        ],\n",
       "       [ 336.45236206,    0.        ],\n",
       "       [ 185.18772888,    0.        ],\n",
       "       [ 336.45236206,    0.        ],\n",
       "       [ 481.52926636,    0.        ],\n",
       "       [ 236.81265259,    0.        ],\n",
       "       [  36.28363419,    0.        ],\n",
       "       [ 385.15914917,    0.        ],\n",
       "       [ 185.18772888,    0.        ],\n",
       "       [ 132.7492218 ,    0.        ],\n",
       "       [ 385.15914917,    0.        ],\n",
       "       [ 236.81265259,    0.        ],\n",
       "       [ 132.7492218 ,    0.        ],\n",
       "       [  81.57950592,    0.        ]], dtype=float32)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin training\n",
      "epoch:0, batch:0, loss:2.15e+04\n",
      "epoch:0, batch:100, loss:1.78e+04\n",
      "epoch:0, batch:200, loss:3.4e+03\n",
      "epoch:0, batch:300, loss:263\n",
      "epoch:1, batch:0, loss:329\n",
      "epoch:1, batch:100, loss:317\n",
      "epoch:1, batch:200, loss:240\n",
      "epoch:1, batch:300, loss:100\n",
      "epoch:2, batch:0, loss:84.8\n",
      "epoch:2, batch:100, loss:42.6\n",
      "epoch:2, batch:200, loss:43.1\n",
      "epoch:2, batch:300, loss:74.7\n",
      "epoch:3, batch:0, loss:96.7\n",
      "epoch:3, batch:100, loss:53.1\n",
      "epoch:3, batch:200, loss:112\n",
      "epoch:3, batch:300, loss:180\n",
      "epoch:4, batch:0, loss:83.6\n",
      "epoch:4, batch:100, loss:83.7\n",
      "epoch:4, batch:200, loss:120\n",
      "epoch:4, batch:300, loss:207\n",
      "epoch:5, batch:0, loss:39.6\n",
      "epoch:5, batch:100, loss:37.5\n",
      "epoch:5, batch:200, loss:360\n",
      "epoch:5, batch:300, loss:261\n",
      "epoch:6, batch:0, loss:43.9\n",
      "epoch:6, batch:100, loss:48.1\n",
      "epoch:6, batch:200, loss:75.8\n",
      "epoch:6, batch:300, loss:101\n",
      "epoch:7, batch:0, loss:118\n",
      "epoch:7, batch:100, loss:170\n",
      "epoch:7, batch:200, loss:113\n",
      "epoch:7, batch:300, loss:78.6\n",
      "epoch:8, batch:0, loss:56.6\n",
      "epoch:8, batch:100, loss:211\n",
      "epoch:8, batch:200, loss:71.2\n",
      "epoch:8, batch:300, loss:232\n",
      "epoch:9, batch:0, loss:66.8\n",
      "epoch:9, batch:100, loss:19.5\n",
      "epoch:9, batch:200, loss:162\n",
      "epoch:9, batch:300, loss:323\n",
      "epoch:10, batch:0, loss:125\n",
      "epoch:10, batch:100, loss:35.1\n",
      "epoch:10, batch:200, loss:89.2\n",
      "epoch:10, batch:300, loss:73.1\n",
      "epoch:11, batch:0, loss:24.9\n",
      "epoch:11, batch:100, loss:23.5\n",
      "epoch:11, batch:200, loss:14.2\n",
      "epoch:11, batch:300, loss:51.4\n",
      "epoch:12, batch:0, loss:86.5\n",
      "epoch:12, batch:100, loss:219\n",
      "epoch:12, batch:200, loss:77.2\n",
      "epoch:12, batch:300, loss:13.7\n",
      "epoch:13, batch:0, loss:71.6\n",
      "epoch:13, batch:100, loss:149\n",
      "epoch:13, batch:200, loss:43\n",
      "epoch:13, batch:300, loss:137\n",
      "epoch:14, batch:0, loss:35.1\n",
      "epoch:14, batch:100, loss:10.5\n",
      "epoch:14, batch:200, loss:38.5\n",
      "epoch:14, batch:300, loss:22.6\n",
      "epoch:15, batch:0, loss:25.4\n",
      "epoch:15, batch:100, loss:69.3\n",
      "epoch:15, batch:200, loss:57.5\n",
      "epoch:15, batch:300, loss:82.5\n",
      "epoch:16, batch:0, loss:145\n",
      "epoch:16, batch:100, loss:13.5\n",
      "epoch:16, batch:200, loss:167\n",
      "epoch:16, batch:300, loss:170\n",
      "epoch:17, batch:0, loss:244\n",
      "epoch:17, batch:100, loss:73.1\n",
      "epoch:17, batch:200, loss:40.7\n",
      "epoch:17, batch:300, loss:79.1\n",
      "epoch:18, batch:0, loss:26.6\n",
      "epoch:18, batch:100, loss:23.9\n",
      "epoch:18, batch:200, loss:57.9\n",
      "epoch:18, batch:300, loss:49\n",
      "epoch:19, batch:0, loss:12\n",
      "epoch:19, batch:100, loss:24.2\n",
      "epoch:19, batch:200, loss:30.6\n",
      "epoch:19, batch:300, loss:342\n",
      "epoch:20, batch:0, loss:33.6\n",
      "epoch:20, batch:100, loss:25.8\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    print ('Begin training')\n",
    "    \n",
    "    for c_epoch in range(n_epochs):\n",
    "        df_epoch_samples = df_train[seq_cols].sample(frac=1)\n",
    "        for c_batch in range(n_batches): \n",
    "            batch = df_epoch_samples[c_batch*batch_size:(c_batch+1)*batch_size].values\n",
    "            batch = batch.reshape(len(batch), seq_length, 1)\n",
    "            results = sess.run([optimizer, loss, latent_vector_in, output_layer], {inputs:batch})\n",
    "            \n",
    "            if c_batch%100==0:\n",
    "                print ('epoch:%d, batch:%d, loss:%.3g' % (c_epoch, c_batch, results[1]))\n",
    "    \n",
    "    latent_vectors = sess.run(latent_vector_in, {inputs:df_train[seq_cols].values.reshape(df_train.shape[0], seq_length, 1)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vectors = pd.DataFrame(latent_vectors, columns=['x', 'y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_seqs_vectors = df_seqs.merge(df_vectors, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "palette = itertools.cycle(sns.color_palette())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_seqs = df_seqs_vectors['label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>label</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0_2.0_2.0_2.0_2.0</td>\n",
       "      <td>1.971067</td>\n",
       "      <td>0.724874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0_2.0_2.0_2.0_2.0</td>\n",
       "      <td>3.419610</td>\n",
       "      <td>0.721450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0_2.0_2.0_2.0_2.0</td>\n",
       "      <td>1.038963</td>\n",
       "      <td>0.655426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0_2.0_2.0_2.0_2.0</td>\n",
       "      <td>3.419610</td>\n",
       "      <td>0.721450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0_2.0_2.0_2.0_2.0</td>\n",
       "      <td>2.721335</td>\n",
       "      <td>0.754503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0_2.0_2.0_2.0_2.0</td>\n",
       "      <td>2.721335</td>\n",
       "      <td>0.754503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0_2.0_2.0_2.0_2.0</td>\n",
       "      <td>2.721335</td>\n",
       "      <td>0.754503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0_2.0_2.0_2.0_2.0</td>\n",
       "      <td>2.721335</td>\n",
       "      <td>0.754503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0_2.0_2.0_2.0_2.0</td>\n",
       "      <td>3.419610</td>\n",
       "      <td>0.721450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0_2.0_2.0_2.0_2.0</td>\n",
       "      <td>4.122154</td>\n",
       "      <td>0.643280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0_2.0_2.0_2.0_2.0</td>\n",
       "      <td>2.721335</td>\n",
       "      <td>0.754503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0_2.0_2.0_2.0_2.0</td>\n",
       "      <td>2.721335</td>\n",
       "      <td>0.754503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0_2.0_2.0_2.0_2.0</td>\n",
       "      <td>1.971067</td>\n",
       "      <td>0.724874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0_2.0_2.0_2.0_2.0</td>\n",
       "      <td>1.971067</td>\n",
       "      <td>0.724874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0_2.0_2.0_2.0_2.0</td>\n",
       "      <td>1.038963</td>\n",
       "      <td>0.655426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0_2.0_2.0_2.0_2.0</td>\n",
       "      <td>2.721335</td>\n",
       "      <td>0.754503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0_2.0_2.0_2.0_2.0</td>\n",
       "      <td>3.419610</td>\n",
       "      <td>0.721450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0_2.0_2.0_2.0_2.0</td>\n",
       "      <td>1.038963</td>\n",
       "      <td>0.655426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0_2.0_2.0_2.0_2.0</td>\n",
       "      <td>2.721335</td>\n",
       "      <td>0.754503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0_2.0_2.0_2.0_2.0</td>\n",
       "      <td>1.971067</td>\n",
       "      <td>0.724874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0_2.0_2.0_2.0_2.0</td>\n",
       "      <td>1.971067</td>\n",
       "      <td>0.724874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0_2.0_2.0_2.0_2.0</td>\n",
       "      <td>3.419610</td>\n",
       "      <td>0.721450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0_2.0_2.0_2.0_2.0</td>\n",
       "      <td>1.971067</td>\n",
       "      <td>0.724874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0_2.0_2.0_2.0_2.0</td>\n",
       "      <td>3.419610</td>\n",
       "      <td>0.721450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0_2.0_2.0_2.0_2.0</td>\n",
       "      <td>1.971067</td>\n",
       "      <td>0.724874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0_2.0_2.0_2.0_2.0</td>\n",
       "      <td>1.038963</td>\n",
       "      <td>0.655426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0_2.0_2.0_2.0_2.0</td>\n",
       "      <td>1.971067</td>\n",
       "      <td>0.724874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0_2.0_2.0_2.0_2.0</td>\n",
       "      <td>3.419610</td>\n",
       "      <td>0.721450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0_2.0_2.0_2.0_2.0</td>\n",
       "      <td>3.419610</td>\n",
       "      <td>0.721450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0_2.0_2.0_2.0_2.0</td>\n",
       "      <td>4.122154</td>\n",
       "      <td>0.643280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39810</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0_2.0_2.0_2.0_2.0</td>\n",
       "      <td>3.419610</td>\n",
       "      <td>0.721450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39817</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0_2.0_2.0_2.0_2.0</td>\n",
       "      <td>1.038963</td>\n",
       "      <td>0.655426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39818</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0_2.0_2.0_2.0_2.0</td>\n",
       "      <td>1.038963</td>\n",
       "      <td>0.655426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39827</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0_2.0_2.0_2.0_2.0</td>\n",
       "      <td>2.721335</td>\n",
       "      <td>0.754503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39828</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0_2.0_2.0_2.0_2.0</td>\n",
       "      <td>3.419610</td>\n",
       "      <td>0.721450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39829</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0_2.0_2.0_2.0_2.0</td>\n",
       "      <td>2.721335</td>\n",
       "      <td>0.754503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39839</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0_2.0_2.0_2.0_2.0</td>\n",
       "      <td>1.038963</td>\n",
       "      <td>0.655426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39847</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0_2.0_2.0_2.0_2.0</td>\n",
       "      <td>2.721335</td>\n",
       "      <td>0.754503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39861</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0_2.0_2.0_2.0_2.0</td>\n",
       "      <td>4.122154</td>\n",
       "      <td>0.643280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39872</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0_2.0_2.0_2.0_2.0</td>\n",
       "      <td>1.038963</td>\n",
       "      <td>0.655426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39886</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0_2.0_2.0_2.0_2.0</td>\n",
       "      <td>1.971067</td>\n",
       "      <td>0.724874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39887</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0_2.0_2.0_2.0_2.0</td>\n",
       "      <td>1.971067</td>\n",
       "      <td>0.724874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39896</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0_2.0_2.0_2.0_2.0</td>\n",
       "      <td>2.721335</td>\n",
       "      <td>0.754503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39897</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0_2.0_2.0_2.0_2.0</td>\n",
       "      <td>1.038963</td>\n",
       "      <td>0.655426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39910</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0_2.0_2.0_2.0_2.0</td>\n",
       "      <td>1.038963</td>\n",
       "      <td>0.655426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39916</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0_2.0_2.0_2.0_2.0</td>\n",
       "      <td>4.122154</td>\n",
       "      <td>0.643280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39919</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0_2.0_2.0_2.0_2.0</td>\n",
       "      <td>3.419610</td>\n",
       "      <td>0.721450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39922</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0_2.0_2.0_2.0_2.0</td>\n",
       "      <td>1.038963</td>\n",
       "      <td>0.655426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39925</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0_2.0_2.0_2.0_2.0</td>\n",
       "      <td>1.971067</td>\n",
       "      <td>0.724874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39929</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0_2.0_2.0_2.0_2.0</td>\n",
       "      <td>1.971067</td>\n",
       "      <td>0.724874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39954</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0_2.0_2.0_2.0_2.0</td>\n",
       "      <td>1.971067</td>\n",
       "      <td>0.724874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39956</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0_2.0_2.0_2.0_2.0</td>\n",
       "      <td>1.038963</td>\n",
       "      <td>0.655426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39957</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0_2.0_2.0_2.0_2.0</td>\n",
       "      <td>3.419610</td>\n",
       "      <td>0.721450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39960</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0_2.0_2.0_2.0_2.0</td>\n",
       "      <td>2.721335</td>\n",
       "      <td>0.754503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39964</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0_2.0_2.0_2.0_2.0</td>\n",
       "      <td>3.419610</td>\n",
       "      <td>0.721450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39974</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0_2.0_2.0_2.0_2.0</td>\n",
       "      <td>4.122154</td>\n",
       "      <td>0.643280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39976</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0_2.0_2.0_2.0_2.0</td>\n",
       "      <td>4.122154</td>\n",
       "      <td>0.643280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39977</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0_2.0_2.0_2.0_2.0</td>\n",
       "      <td>3.419610</td>\n",
       "      <td>0.721450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39990</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0_2.0_2.0_2.0_2.0</td>\n",
       "      <td>1.971067</td>\n",
       "      <td>0.724874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39996</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0_2.0_2.0_2.0_2.0</td>\n",
       "      <td>1.038963</td>\n",
       "      <td>0.655426</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7975 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0    1    2    3    4                label         x         y\n",
       "0      2.0  2.0  2.0  2.0  2.0  2.0_2.0_2.0_2.0_2.0  1.971067  0.724874\n",
       "2      2.0  2.0  2.0  2.0  2.0  2.0_2.0_2.0_2.0_2.0  3.419610  0.721450\n",
       "16     2.0  2.0  2.0  2.0  2.0  2.0_2.0_2.0_2.0_2.0  1.038963  0.655426\n",
       "22     2.0  2.0  2.0  2.0  2.0  2.0_2.0_2.0_2.0_2.0  3.419610  0.721450\n",
       "39     2.0  2.0  2.0  2.0  2.0  2.0_2.0_2.0_2.0_2.0  2.721335  0.754503\n",
       "42     2.0  2.0  2.0  2.0  2.0  2.0_2.0_2.0_2.0_2.0  2.721335  0.754503\n",
       "46     2.0  2.0  2.0  2.0  2.0  2.0_2.0_2.0_2.0_2.0  2.721335  0.754503\n",
       "68     2.0  2.0  2.0  2.0  2.0  2.0_2.0_2.0_2.0_2.0  2.721335  0.754503\n",
       "75     2.0  2.0  2.0  2.0  2.0  2.0_2.0_2.0_2.0_2.0  3.419610  0.721450\n",
       "76     2.0  2.0  2.0  2.0  2.0  2.0_2.0_2.0_2.0_2.0  4.122154  0.643280\n",
       "79     2.0  2.0  2.0  2.0  2.0  2.0_2.0_2.0_2.0_2.0  2.721335  0.754503\n",
       "80     2.0  2.0  2.0  2.0  2.0  2.0_2.0_2.0_2.0_2.0  2.721335  0.754503\n",
       "82     2.0  2.0  2.0  2.0  2.0  2.0_2.0_2.0_2.0_2.0  1.971067  0.724874\n",
       "87     2.0  2.0  2.0  2.0  2.0  2.0_2.0_2.0_2.0_2.0  1.971067  0.724874\n",
       "94     2.0  2.0  2.0  2.0  2.0  2.0_2.0_2.0_2.0_2.0  1.038963  0.655426\n",
       "105    2.0  2.0  2.0  2.0  2.0  2.0_2.0_2.0_2.0_2.0  2.721335  0.754503\n",
       "106    2.0  2.0  2.0  2.0  2.0  2.0_2.0_2.0_2.0_2.0  3.419610  0.721450\n",
       "108    2.0  2.0  2.0  2.0  2.0  2.0_2.0_2.0_2.0_2.0  1.038963  0.655426\n",
       "114    2.0  2.0  2.0  2.0  2.0  2.0_2.0_2.0_2.0_2.0  2.721335  0.754503\n",
       "132    2.0  2.0  2.0  2.0  2.0  2.0_2.0_2.0_2.0_2.0  1.971067  0.724874\n",
       "136    2.0  2.0  2.0  2.0  2.0  2.0_2.0_2.0_2.0_2.0  1.971067  0.724874\n",
       "140    2.0  2.0  2.0  2.0  2.0  2.0_2.0_2.0_2.0_2.0  3.419610  0.721450\n",
       "141    2.0  2.0  2.0  2.0  2.0  2.0_2.0_2.0_2.0_2.0  1.971067  0.724874\n",
       "142    2.0  2.0  2.0  2.0  2.0  2.0_2.0_2.0_2.0_2.0  3.419610  0.721450\n",
       "145    2.0  2.0  2.0  2.0  2.0  2.0_2.0_2.0_2.0_2.0  1.971067  0.724874\n",
       "148    2.0  2.0  2.0  2.0  2.0  2.0_2.0_2.0_2.0_2.0  1.038963  0.655426\n",
       "149    2.0  2.0  2.0  2.0  2.0  2.0_2.0_2.0_2.0_2.0  1.971067  0.724874\n",
       "167    2.0  2.0  2.0  2.0  2.0  2.0_2.0_2.0_2.0_2.0  3.419610  0.721450\n",
       "168    2.0  2.0  2.0  2.0  2.0  2.0_2.0_2.0_2.0_2.0  3.419610  0.721450\n",
       "176    2.0  2.0  2.0  2.0  2.0  2.0_2.0_2.0_2.0_2.0  4.122154  0.643280\n",
       "...    ...  ...  ...  ...  ...                  ...       ...       ...\n",
       "39810  2.0  2.0  2.0  2.0  2.0  2.0_2.0_2.0_2.0_2.0  3.419610  0.721450\n",
       "39817  2.0  2.0  2.0  2.0  2.0  2.0_2.0_2.0_2.0_2.0  1.038963  0.655426\n",
       "39818  2.0  2.0  2.0  2.0  2.0  2.0_2.0_2.0_2.0_2.0  1.038963  0.655426\n",
       "39827  2.0  2.0  2.0  2.0  2.0  2.0_2.0_2.0_2.0_2.0  2.721335  0.754503\n",
       "39828  2.0  2.0  2.0  2.0  2.0  2.0_2.0_2.0_2.0_2.0  3.419610  0.721450\n",
       "39829  2.0  2.0  2.0  2.0  2.0  2.0_2.0_2.0_2.0_2.0  2.721335  0.754503\n",
       "39839  2.0  2.0  2.0  2.0  2.0  2.0_2.0_2.0_2.0_2.0  1.038963  0.655426\n",
       "39847  2.0  2.0  2.0  2.0  2.0  2.0_2.0_2.0_2.0_2.0  2.721335  0.754503\n",
       "39861  2.0  2.0  2.0  2.0  2.0  2.0_2.0_2.0_2.0_2.0  4.122154  0.643280\n",
       "39872  2.0  2.0  2.0  2.0  2.0  2.0_2.0_2.0_2.0_2.0  1.038963  0.655426\n",
       "39886  2.0  2.0  2.0  2.0  2.0  2.0_2.0_2.0_2.0_2.0  1.971067  0.724874\n",
       "39887  2.0  2.0  2.0  2.0  2.0  2.0_2.0_2.0_2.0_2.0  1.971067  0.724874\n",
       "39896  2.0  2.0  2.0  2.0  2.0  2.0_2.0_2.0_2.0_2.0  2.721335  0.754503\n",
       "39897  2.0  2.0  2.0  2.0  2.0  2.0_2.0_2.0_2.0_2.0  1.038963  0.655426\n",
       "39910  2.0  2.0  2.0  2.0  2.0  2.0_2.0_2.0_2.0_2.0  1.038963  0.655426\n",
       "39916  2.0  2.0  2.0  2.0  2.0  2.0_2.0_2.0_2.0_2.0  4.122154  0.643280\n",
       "39919  2.0  2.0  2.0  2.0  2.0  2.0_2.0_2.0_2.0_2.0  3.419610  0.721450\n",
       "39922  2.0  2.0  2.0  2.0  2.0  2.0_2.0_2.0_2.0_2.0  1.038963  0.655426\n",
       "39925  2.0  2.0  2.0  2.0  2.0  2.0_2.0_2.0_2.0_2.0  1.971067  0.724874\n",
       "39929  2.0  2.0  2.0  2.0  2.0  2.0_2.0_2.0_2.0_2.0  1.971067  0.724874\n",
       "39954  2.0  2.0  2.0  2.0  2.0  2.0_2.0_2.0_2.0_2.0  1.971067  0.724874\n",
       "39956  2.0  2.0  2.0  2.0  2.0  2.0_2.0_2.0_2.0_2.0  1.038963  0.655426\n",
       "39957  2.0  2.0  2.0  2.0  2.0  2.0_2.0_2.0_2.0_2.0  3.419610  0.721450\n",
       "39960  2.0  2.0  2.0  2.0  2.0  2.0_2.0_2.0_2.0_2.0  2.721335  0.754503\n",
       "39964  2.0  2.0  2.0  2.0  2.0  2.0_2.0_2.0_2.0_2.0  3.419610  0.721450\n",
       "39974  2.0  2.0  2.0  2.0  2.0  2.0_2.0_2.0_2.0_2.0  4.122154  0.643280\n",
       "39976  2.0  2.0  2.0  2.0  2.0  2.0_2.0_2.0_2.0_2.0  4.122154  0.643280\n",
       "39977  2.0  2.0  2.0  2.0  2.0  2.0_2.0_2.0_2.0_2.0  3.419610  0.721450\n",
       "39990  2.0  2.0  2.0  2.0  2.0  2.0_2.0_2.0_2.0_2.0  1.971067  0.724874\n",
       "39996  2.0  2.0  2.0  2.0  2.0  2.0_2.0_2.0_2.0_2.0  1.038963  0.655426\n",
       "\n",
       "[7975 rows x 8 columns]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_seqs_vectors[df_seqs_vectors[0]==2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAl0AAAJCCAYAAAD6AnJlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAIABJREFUeJzt3Xt0VdXZ/v3rliCoIGcUCJbAjhaR\nABKI1B4ESgW1obbhYPtIbLFSRWl53nroa9W+tgykB1v9GXHwyE/B9gErVhIrQhXsyYoQhFbBajgp\nCVEwElBLgMD9/pEQc9g72Tkwk8D3M8YeZM01952551jC5Vxrr2XuLgAAAJxYpzX3AAAAAE4FhC4A\nAIAACF0AAAABELoAAAACIHQBAAAEQOgCAAAIgNAFAAAQAKELAAAgAEIXAABAAAnNPYDqunfv7v36\n9WvuYQAAANRpw4YNH7h7j3j6trjQ1a9fP+Xm5jb3MAAAAOpkZu/E25fTiwAAAAEQugAAAAIgdAEA\nAATQ4q7pAgCgOR05ckT5+fkqKSlp7qGgBWnfvr0SExPVtm3bBtcgdAEAUEl+fr46duyofv36ycya\nezhoAdxdRUVFys/PV1JSUoPrcHoRAIBKSkpK1K1bNwIXKpiZunXr1ujVT0IXAADVELhQXVMcE4Qu\nAACAAAhdAAAAARC6AABoYXbt2qXRo0dr4MCBGjRokB544IEafdxds2bNUiQSUUpKil577bWY9TZt\n2qRRo0Zp0KBBSklJ0ZNPPhm136FDhzRlyhRFIhGlpaVp586dMWu+8MILGj58uAYPHqzhw4drzZo1\nUft9+OGHGjdunJKTkzVu3Djt27cvZs3f/e53SklJUUpKij73uc/pn//8Z9R+O3bsUFpampKTkzVl\nyhQdPnw4Zs37779fF154oVJSUjR27Fi98070G8hv2LBBgwcPViQS0axZs+TuMWs2mLu3qNfw4cMd\nAIDmsmXLlnr1f+a1fP/c3NXe7/Y/+ufmrvZnXstv9Bh2797tGzZscHf3AwcOeHJysm/evLlKn+ee\ne87Hjx/vx44d81deecVHjhwZs95bb73lb7/9tru7FxQU+Lnnnuv79u2r0S8rK8tnzJjh7u5Llizx\nyZMnx6z52muveUFBgbu7v/766967d++o/W699VafO3euu7vPnTvXb7vttpg1X375Zf/www/d3X3F\nihUxP9OkSZN8yZIl7u4+Y8YMf/jhh2PWXLNmjX/yySfu7v7www/H/EwjRozwf/zjH37s2DEfP368\nr1ixokafaMeGpFyPM+Ow0gUAQAMt31igH/3hdRUUH5RLKig+qB/94XUt31jQqLq9evXSxRdfLEnq\n2LGjBg4cqIKCqjWzs7M1bdo0mZkuueQSFRcXq7CwMGq9888/X8nJyZKk3r17q2fPntq7d2+NftnZ\n2crMzJQkZWRkaPXq1TFXfIYNG6bevXtLkgYNGqSSkhIdOnSo1pqZmZlavnx5zM/9uc99Tl26dJEk\nXXLJJcrPz6/Rx921Zs0aZWRkxFVz9OjROvPMM2utWVhYqAMHDmjUqFEyM02bNq3Wmg1F6AIAoIF+\nseotHTxytErbwSNH9YtVbzXZ79i5c6c2btyotLS0Ku0FBQXq27dvxXZiYmKNYBbNunXrdPjwYQ0Y\nMKDGvso1ExIS1KlTJxUVFdVZ8+mnn9awYcPUrl27Gvvef/999erVS1JZmNyzZ0+d9SRp4cKFmjBh\nQo32oqIide7cWQkJZbcajfdz11azoKBAiYmJFdv1qVkf3BwVAIAG2l18sF7t9fXxxx/rG9/4hn7z\nm9/o7LPPrrIv2gpUXbc1KCws1LXXXqtFixbptNNqrrs0pObmzZt1++23609/+lOt/erjpZde0sKF\nC/X3v/+9ScYoSb/97W+Vm5urv/zlL01Ws75Y6QIAoIF6dz6jXu31ceTIEX3jG9/Qt771LX3961+v\nsT8xMVG7du2q2M7Pz6843RfNgQMHdOWVV+pnP/uZLrnkkqh9KtcsLS3V/v371bVr15g18/PzdfXV\nV2vx4sVRV84k6Zxzzqk47VlYWKiePXvGrCdJ//rXv3T99dcrOztb3bp1q7G/e/fuKi4uVmlpacUY\navvckvTiiy9qzpw5ysnJiboal5iYWOW0Yzw1G4LQBQBAA916+QU6o22bKm1ntG2jWy+/oFF13V3T\np0/XwIED9d///d9R+6Snp2vx4sVyd61du1adOnWqOI1X3eHDh3X11Vdr2rRpmjRpUszfm56erkWL\nFkmSli1bpjFjxsRc8SkuLtaVV16puXPn6tJLL42r5qJFizRx4sSYfd999119/etf1xNPPKHzzz8/\nah8z0+jRo7Vs2bK4am7cuFEzZsxQTk5OzMDXq1cvdezYUWvXrpW7a/HixbXWbLB4r7gP9eLbiwCA\n5tQSvr34t7/9zSX54MGDfciQIT5kyBB/7rnnfP78+T5//nx3dz927JjfdNNN3r9/f7/ooot8/fr1\nMes98cQTnpCQUFFryJAhvnHjRnd3v+uuuzw7O9vd3Q8ePOgZGRk+YMAAHzFihG/bti1mzZ/+9Kd+\n5plnVqn5/vvvu7v79OnTK8bzwQcf+JgxYzwSifiYMWO8qKgoZs3p06d7586dK+pVzgQTJkyo+Lbk\ntm3bfMSIET5gwADPyMjwkpKSmDXHjh3rPXv2rKj51a9+tWLfkCFDKn5ev369Dxo0yPv37+8zZ870\nY8eO1ajV2G8vmp+I+1A0Qmpqqufm5jb3MAAAp6g333xTAwcObO5hoAWKdmyY2QZ3T43n/ZxeBAAA\nCCCuby+a2XhJD0hqI+lRd7+v2v5fSxpdvnmmpJ7u3rl833mSHpXUV5JLusLddzbJ6AEAQIXXX39d\n1157bZW2du3a6dVXX21wzVWrVun222+v0paUlKRnnnmmwTUfe+yxGnfZv/TSS5WVldXgmnPmzNFT\nTz1VpW3SpEm68847G1yzqdV5etHM2kh6W9I4SfmS1ku6xt23xOh/i6Rh7v6d8u0/S5rj7i+YWQdJ\nx9z9P7F+H6cXAQDNidOLiCXE6cWRkra6+3Z3PyxpqaTaLum/RtKS8oFcKCnB3V+QJHf/uLbABQAA\ncLKKJ3T1kbSr0nZ+eVsNZvYZSUmSjj/18nxJxWb2BzPbaGa/KF85AwAAOKXEE7qi3aAj1jnJqZKW\nufvxZyIkSPqCpB9KGiGpv6TravwCsxvMLNfMcqM9CwoAAKC1iyd05avsIvjjEiXtjtF3qspPLVZ6\n78byU5OlkpZLurj6m9x9gbununtqjx494hs5AABAKxJP6FovKdnMkszsdJUFq5zqnczsAkldJL1S\n7b1dzOx4khojKeoF+AAAtEr/+r3064ukn3Qu+/Nfv2+y0kePHtWwYcN01VVX1dh36NAhTZkyRZFI\nRGlpadq5c2ed9d5991116NBBv/zlL6Pu37Fjh9LS0pScnKwpU6bo8OHDddZctmyZzEyxvgS3cuVK\nXXDBBYpEIrrvvvui9qnulltuUYcOHWLunzt3riKRiC644AKtWrWqznq1zaPUsLlsiDpDV/kK1c2S\nVkl6U9Lv3X2zmd1rZumVul4jaalX+jpk+WnGH0pabWavq+xU5f805QcAAKDZ/Ov30rOzpP27JHnZ\nn8/OarLg9cADD8T8JuXChQvVpUsXbd26VbNnz65xW4doZs+erQkTJsTcf/vtt2v27NnKy8tTly5d\ntHDhwlrrffTRR3rwwQeVlpYWdf/Ro0c1c+ZMPf/889qyZYuWLFmiLVtqX3vJzc1VcXFxzP1btmzR\n0qVLtXnzZq1cuVI33XSTjh49GrO/VPs8Sg2by4aI6+ao7r7C3c939wHuPqe87W53z6nU5yfufkeU\n977g7inuPtjdryv/BiQAAK3f6nulIwerth05WNbeSPn5+Xruued0/fXXR92fnZ2tzMxMSVJGRoZW\nr16t2m4DtXz5cvXv31+DBg2Kut/dtWbNGmVkZEiSMjMztXz58lrHeNddd+m2225T+/bto+5ft26d\nIpGI+vfvr9NPP11Tp05VdnZ2zHpHjx7Vrbfeqp///Ocx+2RnZ2vq1Klq166dkpKSFIlEtG7dupj9\n65rH4zXrM5cNxR3pAQBoqP359Wuvhx/84Af6+c9/rtNOi/5PdUFBgfr2LbvkOiEhQZ06dVJRUVHU\nvp988onmzZune+65J+bvKyoqUufOnZWQUHbf9MTERBUUFMTsv3HjRu3atSvmKbvqY4yn5kMPPaT0\n9PSYD+5uSM265rF6zbrmsjEIXQAANFSnxPq1x+mPf/yjevbsqeHDh8fsE20lxizaDQeke+65R7Nn\nz671Oqn61Dt27Jhmz56tX/3qVzHr1bfm7t279dRTT+mWW25psprxzGN9azZGXI8BAgAAUYy9u+wa\nrsqnGNueUdbeCC+//LJycnK0YsUKlZSU6MCBA/qv//ov/fa3v63ok5iYqF27dikxMVGlpaXav3+/\nunbtGrXeq6++qmXLlum2225TcXGxTjvtNLVv314333xzRZ/u3buruLhYpaWlSkhIUH5+vnr37h21\n3kcffaQ33nhDl112mSTpvffeU3p6unJycpSa+unN2Y+P8bjaam7cuFFbt25VJBKRJP3nP/9RJBLR\n1q1bq/SrT8145rFyzXjmslHcvUW9hg8f7gAANJctW7bU7w3/fNL9/kHu93Qq+/OfTzbpeF566SW/\n8sora7Q/9NBDPmPGDHd3X7JkiU+aNCmuevfcc4//4he/iLovIyPDlyxZ4u7uM2bM8KysrLhqfulL\nX/L169fXaD9y5IgnJSX59u3b/dChQ56SkuJvvPFGXDXPOuusqO1vvPGGp6SkeElJiW/fvt2TkpK8\ntLS0znqx5tE9/rmMdmxIyvU4Mw6nFwEAaIyUydLsN6SfFJf9mTL5hP2qu+++Wzk5Zd9hmz59uoqK\nihSJRHT//ffHfTuG6q644grt3l12+8158+bp/vvvVyQSUVFRkaZPn17vert379YVV1whqez6qIce\nekiXX365Bg4cqMmTJ8e8kL82OTk5uvvustXDQYMGafLkybrwwgs1fvx4ZWVlqU2b+j/s5kTMZV3q\nfOB1aDzwGgDQnHjgNWIJ8cBrAAAANBIX0gMAcJJYtWpVjRt7JiUl6ZlnnmlwzTlz5uipp56q0jZp\n0iTdeeedDa559dVXa8eOHVXa5s2bp8svv7xB9YqKijR27Nga7atXr1a3bt0aVPNE4PQiAACVcHoR\nsXB6EQAAoBUgdAEAAARA6AIAAAiA0AUAQAvUr18/DR48WEOHDq1yl/fj3F2zZs1SJBJRSkqKXnvt\ntVrrtWnTRkOHDtXQoUOVnp4etc+hQ4c0ZcoURSIRpaWlaefOnTHr7dy5U2eccUZFze9973tR+334\n4YcaN26ckpOTNW7cOO3bty9mzT//+c/q1KlTRc17743+4PAdO3YoLS1NycnJmjJlig4fPhyz5uOP\nP64ePXpU1Hz00Uej9tuwYYMGDx6sSCSiWbNm8cBrAABamue2P6evLPuKUhal6CvLvqLntj/XZLVf\neuklbdq0SdG+YPb8888rLy9PeXl5WrBggW688cZaa51xxhnatGmTNm3aVHFT0OoWLlyoLl26aOvW\nrZo9e3aNb0JWN2DAgIqajzzySNQ+9913n8aOHau8vDyNHTu2zhuPfuELX6ioefyGqNXdfvvtmj17\ntvLy8tSlSxctXLiw1ppTpkypqHn99ddH7XPjjTdqwYIFFXO6cuXKWms2BKELABoo64YXlTVj9aev\nG15s7iEhsOe2P6ef/OMnKvykUC5X4SeF+sk/ftKkwSuW7OxsTZs2TWamSy65RMXFxSosLGx0zczM\nTElSRkaGVq9e3egVn8o1MzMztXz58kbVc3etWbNGGRkZTVazsLBQBw4c0KhRo2RmmjZtWqNrRkPo\nAoAGyLrhRcmsxovgdWp54LUHVHK0pEpbydESPfDaA42ubWb6yle+ouHDh2vBggU19hcUFKhv374V\n24mJiSooKIhZr6SkRKmpqbrkkktiBorKNRMSEtSpUycVFRXFrLljxw4NGzZMX/rSl/S3v/0tap/3\n339fvXr1kiT16tVLe/bsiVlPkl555RUNGTJEEyZM0ObNm2vsLyoqUufOnZWQUHar0bo+tyQ9/fTT\nSklJUUZGRpWHZR9XUFCgxMTEiu14ajYEN0cFgIY4HrSqt+GU8t4n79WrvT5efvll9e7dW3v27NG4\nceP02c9+Vl/84hcr9kdbgbJajsF3331XvXv31vbt2zVmzBgNHjxYAwYMqNKnPjV79eqld999V926\nddOGDRv0ta99TZs3b9bZZ58d70es4eKLL9Y777yjDh06aMWKFfra176mvLy8Bo9Rkr761a/qmmuu\nUbt27fTII48oMzNTa9asaVTNhmKlCwCABjr3rHPr1V4fvXv3liT17NlTV199tdatW1dlf2JiYpVV\nm/z8/Ir31Favf//+uuyyy7Rx48YafSrXLC0t1f79+9W1a9eo9dq1a1dxt/fhw4drwIABevvtt2v0\nO+eccypOexYWFqpnz54xx3j22WerQ4cOksoexH3kyBF98MEHVfp0795dxcXFKi0tjetzd+vWTe3a\ntZMkffe739WGDRuifu78/PyK7bpqNhShCwCABvr+xd9X+zbtq7S1b9Ne37/4+42q+8knn+ijjz6q\n+PlPf/qTLrrooip90tPTtXjxYrm71q5dq06dOlWcxqtu3759OnTokCTpgw8+0Msvv6wLL7ywRr/0\n9HQtWrRIkrRs2TKNGTMm5orP3r17dfToUUnS9u3blZeXp/79+9dac9GiRZo4cWLMz/3ee+9VrDqt\nW7dOx44dq/EYHzPT6NGjtWzZsrhqVr7OLScnJ+rTBnr16qWOHTtq7dq1cnctXry41poNxelFAGiI\n46cjKv+D5P5pO04JV/a/UlLZtV3vffKezj3rXH3/4u9XtDfU+++/r6uvvlpS2YrTN7/5TY0fP77i\nG4Lf+973dMUVV2jFihWKRCI688wz9dhjj8Ws9+abb2rGjBk67bTTdOzYMd1xxx0Voevuu+9Wamqq\n0tPTNX36dF177bWKRCLq2rWrli5dGrPmX//6V919991KSEhQmzZt9Mgjj1Ssil1//fX63ve+p9TU\nVN1xxx2aPHmyFi5cqPPOO6/GcxwrW7ZsmebPn6+EhASdccYZWrp0aUXou+KKK/Too4+qd+/emjdv\nnqZOnaof//jHGjZsmKZPnx6z5oMPPqicnBwlJCSoa9euevzxxyv2DR06VJs2bZIkzZ8/X9ddd50O\nHjyoCRMmaMKECTFrNhTPXgSABqq4mP44d81c8OXmGxCaBM9eRCyNffYiK10A0EAELAD1QegCAOAk\n8frrr+vaa6+t0tauXTu9+uqrDa65atWqGjdJTUpK0jPPPNPgmo899pgeeKDqbTUuvfRSZWVlNbjm\nnDlzapy6nDRpku68884G12xqnF4EAKASTi8ilsaeXuTbiwAAAAEQugAAAAIgdAEAAARA6AIAAAiA\n0AUAQAvzne98Rz179qxxF/rj3F2zZs1SJBJRSkqKXnvttVrrjR8/Xp07d9ZVV10Vs8+hQ4c0ZcoU\nRSIRpaWlaefOnTH7FhUVafTo0erQoYNuvvnmmP0+/PBDjRs3TsnJyRo3bpz27dsXs++///1vjRo1\nSu3atdMvf/nLmP127NihtLQ0JScna8qUKTp8+HDMvn/961918cUXKyEhoeIO9tFs2LBBgwcPViQS\n0axZs6I+i7EpELoAAGiE/c8+q7wxY/XmwAuVN2as9j/7bKNrXnfddVq5cmXM/c8//7zy8vKUl5en\nBQsW6MYbb6y13q233qonnnii1j4LFy5Uly5dtHXrVs2ePbvGbSIqa9++vX7605/WGo4k6b777tPY\nsWOVl5ensWPH6r777ovZt2vXrnrwwQf1wx/+sNaat99+u2bPnq28vDx16dJFCxcujNn3vPPO0+OP\nP65vfvObtda88cYbtWDBgoo5rW3uG4PQBQBAA+1/9lkV3nW3SnfvltxVunu3Cu+6u9HB64tf/GLM\nB01LUnZ2tqZNmyYz0yWXXKLi4uIqzxisbuzYserYsWOtvzM7O1uZmZmSpIyMDK1evTrmis9ZZ52l\nz3/+82rfvn3U/dFqZmZmavny5TH79uzZUyNGjFDbtm1j9nF3rVmzRhkZGXHV7Nevn1JSUnTaabHj\nTmFhoQ4cOKBRo0bJzDRt2rRaazYGoQsAgAba8+vfyEtKqrR5SYn2/Po3J/T3FhQUqG/fvhXbiYmJ\nKigoaLKaCQkJ6tSpk4qKihpV8/333694CHevXr20Z8+eRtUrKipS586dlZBQdm/3pvrciYmJFdtN\nUTMWQhcAAA1UGmN1KVZ7U4m2AmWVnwPaQmo2tdb+uQldAAA0UEL5Kk687U0lMTFRu3btqtjOz89X\n7969m6xmaWmp9u/fX+spznicc845Fac9CwsL1bNnz0bV6969u4qLi1VaWiqp6T53fn5+xXZT1IyF\n0AUAQAP1nP0DWbXrmqx9e/Wc/YMT+nvT09O1ePFiubvWrl2rTp06VZzGa0zNRYsWSZKWLVumMWPG\nNHrFp3LNRYsWaeLEiY2qZ2YaPXp0xTcRm6Jmr1691LFjR61du1bursWLFze6Zkzu3qJew4cPdwAA\nmsuWLVvq1b84J8ffHj3Gt3x2oL89eowX5+Q0egxTp071c8891xMSErxPnz7+6KOP+vz5833+/Pnu\n7n7s2DG/6aabvH///n7RRRf5+vXra633+c9/3rt37+7t27f3Pn36+MqVK93d/a677vLs7Gx3dz94\n8KBnZGT4gAEDfMSIEb5t27Zaa37mM5/xLl26+FlnneV9+vTxzZs3u7v79OnTK8bzwQcf+JgxYzwS\nifiYMWO8qKgoZr3CwkLv06ePd+zY0Tt16uR9+vTx/fv3u7v7hAkTvKCgwN3dt23b5iNGjPABAwZ4\nRkaGl5SUxKy5bt0679Onj5955pnetWtXv/DCCyv2DRkypOLn9evX+6BBg7x///4+c+ZMP3bsWNR6\n0Y4NSbkeZ8bhgdcAAFTCA68RCw+8BgAAaAUSmnsAAACg8V5//XVde+21VdratWunV199tcE1V61a\nVeMmqUlJSXrmmWcaXPOxxx7TAw88UKXt0ksvVVZWVoNrzpkzR0899VSVtkmTJunOO+9scM0TgdOL\nAABUwulFxMLpRQAAgFaA0AUAABAAoQsAACAAQhcAAEAAhC4AAFqYkpISjRw5UkOGDNGgQYN0zz33\n1Ohz6NAhTZkyRZFIRGlpadq5c2fMeuvWrdPQoUM1dOhQDRkyJOa3D3fs2KG0tDQlJydrypQpOnz4\ncMyajzzyiAYPHqyhQ4fq85//vLZs2RK138qVK3XBBRcoEonovvvuq/VzT58+XUOGDFFKSooyMjL0\n8ccfR+03d+5cRSIRXXDBBVq1alXMevHMo1S/uWyUeO+iGurFHekBAM2pvnekf2ttoT/+o7/7QzNW\n++M/+ru/tbaw0WM4duyYf/TRR+7ufvjwYR85cqS/8sorVfpkZWX5jBkz3N19yZIlPnny5Jj1Pvnk\nEz9y5Ii7u+/evdt79OhRsV3ZpEmTfMmSJe7uPmPGDH/44Ydj1jx+t3h39+zsbL/88str9CktLfX+\n/fv7tm3b/NChQ56SklJx5/q6as6ePdvnzp1bo8/mzZs9JSXFS0pKfPv27d6/f38vLS2NWi+eeXSP\nfy4be0d6VroAAGigt199Ty/97t/6+MNDkqSPPzykl373b7396nuNqmtm6tChgyTpyJEjOnLkSI3n\nIGZnZyszM1OSlJGRodWrV8tj3AbqzDPPVEJC2a05S0pKoj5T0d21Zs0aZWRkSJIyMzO1fPnymGM8\n++yzK37+5JNPotZct26dIpGI+vfvr9NPP11Tp05VdnZ2nTXdXQcPHoxaMzs7W1OnTlW7du2UlJSk\nSCSidevWRa0XzzwerxnvXDYGoQsAgAZ6JXubSg8fq9JWeviYXsne1ujaR48e1dChQ9WzZ0+NGzdO\naWlpVfYXFBSob9++kqSEhAR16tRJRUVFMeu9+uqrGjRokAYPHqxHHnmkIoQdV1RUpM6dO1e0JyYm\nqqCgoNYxZmVlacCAAbrtttv04IMP1thfeYzx1vz2t7+tc889V//+9791yy23NLpmXfNYvWY8c9lQ\nhC4AABro+ApXvO310aZNG23atEn5+flat26d3njjjSr7o63ERFvFOS4tLU2bN2/W+vXrNXfuXJWU\nlDSqniTNnDlT27Zt07x58/Szn/2sxv6G1Hzssce0e/duDRw4UE8++WSja9Y1jw0dZ0MQugAAaKAO\nXdvVq70hOnfurMsuu0wrV66s0p6YmKhdu3ZJkkpLS7V//3517dq1znoDBw7UWWedVSN8dO/eXcXF\nxSotLZUk5efnq3fv3nGNcerUqVFPRVYeY31qtmnTRlOmTNHTTz/dZDVjzWP1mvWZy/oidAEA0ECj\nJg5QwulV/ylNOP00jZo4oFF19+7dq+LiYknSwYMH9eKLL+qzn/1slT7p6elatGiRJGnZsmUaM2ZM\nzNWZHTt2VISpd955R2+99Zb69etXpY+ZafTo0Vq2bJkkadGiRZo4cWLMMebl5VX8/Nxzzyk5OblG\nnxEjRigvL087duzQ4cOHtXTpUqWnp0et5+7aunVrxc/PPvtsjc98/HMvXbpUhw4d0o4dO5SXl6eR\nI0dGrRnPPB6vGe9cNgYPvAYAoIHOTztXUtm1XR9/eEgdurbTqIkDKtobqrCwUJmZmTp69KiOHTum\nyZMn66qrrtLdd9+t1NRUpaena/r06br22msViUTUtWtXLV26NGa9v//977rvvvvUtm1bnXbaaXr4\n4YfVvXt3SdIVV1yhRx99VL1799a8efM0depU/fjHP9awYcM0ffr0mDUfeughvfjii2rbtq26dOlS\nEVp2796t66+/XitWrFBCQoIeeughXX755Tp69Ki+853vaNCgQVHrubsyMzN14MABubuGDBmi+fPn\nS5JycnKUm5ure++9V4MGDdLkyZN14YUXKiEhQVlZWWrTpk295lFSg+eyMXjgNQAAlfDAa8TCA68B\nAABaAU4vAgBwkli1apVuv/0nO4JdAAAZSElEQVT2Km1JSUkx70Afjzlz5uipp56q0jZp0iTdeeed\nDa559dVXa8eOHVXa5s2bp8svv7xB9YqKijR27Nga7atXr1a3bt0aVPNE4PQiAACVcHoRsXB6EQCA\nJtbSFiTQ/JrimCB0AQBQSfv27VVUVETwQgV3V1FRkdq3b9+oOlzTBQBAJYmJicrPz9fevXubeyho\nQdq3b6/ExMRG1SB0AQBQSdu2bZWUlNTcw8BJiNOLAAAAARC6AAAAAiB0AQAABEDoAgAACIDQBQAA\nEAChCwAAIABCFwAAQACELgAAgAAIXQAAAAEQugAAAAIgdAEAAARA6AIAAAggrtBlZuPN7C0z22pm\nd0TZ/2sz21T+etvMiqvtP9vMCszsoaYaOAAAQGuSUFcHM2sjKUvSOEn5ktabWY67bznex91nV+p/\ni6Rh1cr8VNJfmmTEAAAArVA8K10jJW119+3ufljSUkkTa+l/jaQlxzfMbLikcyT9qTEDBQAAaM3i\nCV19JO2qtJ1f3laDmX1GUpKkNeXbp0n6laRbGzdMAACA1i2e0GVR2jxG36mSlrn70fLtmyStcPdd\nMfqX/QKzG8ws18xy9+7dG8eQAAAAWpc6r+lS2cpW30rbiZJ2x+g7VdLMStujJH3BzG6S1EHS6Wb2\nsbtXuRjf3RdIWiBJqampsQIdAABAqxVP6FovKdnMkiQVqCxYfbN6JzO7QFIXSa8cb3P3b1Xaf52k\n1OqBCwAA4FRQ5+lFdy+VdLOkVZLelPR7d99sZveaWXqlrtdIWururFQBAABUYy0tI6Wmpnpubm5z\nDwMAAKBOZrbB3VPj6csd6QEAAAIgdAEAAARA6AIAAAiA0AUAABAAoQsAACAAQhcAAEAAhC4AAIAA\nCF0AAAABELoAAAACIHQBAAAEQOgCAAAIgNAFAAAQAKELAAAgAEIXAABAAIQuAACAAAhdAAAAARC6\nAAAAAiB0AQAABEDoAgAACIDQBQAAEAChCwAAIABCFwAAQACELgAAgAAIXQAAAAEQugAAAAIgdAEA\nAARA6AIAAAiA0AUAABAAoQsAACAAQhcAAEAAhC4AAIAACF0AAAABELoAAAACIHQBAAAEQOgCAAAI\ngNAFAAAQAKELAAAgAEIXAABAAIQuAACAAAhdAAAAARC6AAAAAiB0AQAABEDoAgAACIDQBQAAEACh\nCwAAIABCFwAAQACELgAAgAAIXQAAAAEQugAAAAIgdAEAAARA6AIAAAiA0AUAABAAoQsAACCAhOYe\nAHAyyrrhRcns0wZ3zVzw5eYbEACg2bHSBTSxisBV7ZV1w4vNPTQAQDNipQtoaseDVvU2AMApjZUu\nAACAAAhdAAAAARC6gKbmXvaqqw0AcEohdAFNbOaCL38asiq9+PYiAJzauJAeOAEIWACA6ljpAgAA\nCIDQBQAAEAChCwAAIABCFwAAQACELgAAgAAIXQAAAAFwywgAwCmt4iH1x3FfPZwgrHQBAE5ZFYGr\n2ivrhhebe2g4CbHSBQA4dR0PWtXbgBOAlS4AAIAACF0AAAABxBW6zGy8mb1lZlvN7I4o+39tZpvK\nX2+bWXF5+1Aze8XMNpvZv8xsSlN/AAAAGuz4Q+nragOaQJ2hy8zaSMqSNEHShZKuMbMLK/dx99nu\nPtTdh0r6P5L+UL7rP5KmufsgSeMl/cbMOjflBwAAoKFmLvjypyGr0otvL+JEiOdC+pGStrr7dkky\ns6WSJkraEqP/NZLukSR3f/t4o7vvNrM9knpIKm7MoAEAaCoELIQSz+nFPpJ2VdrOL2+rwcw+IylJ\n0poo+0ZKOl3StvoPEwAAoHWLJ3RF++5srJPdUyUtc/ejVQqY9ZL0hKRvu/uxGr/A7AYzyzWz3L17\n98YxJAAAgNYlntCVL6lvpe1ESbtj9J0qaUnlBjM7W9Jzkn7s7mujvcndF7h7qrun9ujRI44hAQAA\ntC7xhK71kpLNLMnMTldZsMqp3snMLpDURdIrldpOl/SMpMXu/lTTDBkAAKD1qTN0uXuppJslrZL0\npqTfu/tmM7vXzNIrdb1G0lL3Kt+znSzpi5Kuq3RLiaFNOH4AAIBWwbyF3YskNTXVc3Nzm3sYAAAA\ndTKzDe6eGk9f7kgPAAAQAKELAAAgAEIXAABAAIQuAACAAAhdAAAAARC6AAAAAiB0AQAABEDoAgAA\nCIDQBQAAEAChCwAAIABCFwAAQACELgAAgAAIXQAAAAEQugAAAAIgdAEAAARA6AIAAAiA0AUAABAA\noQsAACAAQhcAAEAAhC4AAIAACF0AAAABELoAAAACIHQBAAAEQOgCAAAIgNAFAAAQAKELAAAgAEIX\nAABAAIQuAACAAAhdAAAAARC6AAAAAiB0AQAABEDoAgAACIDQBQAAEAChCwAAIABCFwAAQACELgAA\ngAAIXQAAAAEQugAAAAIgdAEAAARA6AIAAAiA0AUAABAAoQsAACAAQhcAAEAAhC4AAIAACF0AAAAB\nELoAAAACIHQBAAAEQOgCAAAIgNAFAAAQAKELAAAgAEIXAABAAIQuAACAAAhdAAAAARC6AAAAAiB0\nAQAABEDoAgAACIDQBQAAEAChCwAAIABCFwAAQACELgAAgAAIXQAAAAEQugAAAAIgdAEAAARA6AIA\nAAiA0AUAABAAoQsAACAAQhcAAEAAhC4AAIAACF0AAAABELoAAAACIHQBAAAEQOgCAAAIIK7QZWbj\nzewtM9tqZndE2f9rM9tU/nrbzIor7cs0s7zyV2ZTDh4AAKC1SKirg5m1kZQlaZykfEnrzSzH3bcc\n7+Pusyv1v0XSsPKfu0q6R1KqJJe0ofy9+5r0UwAAALRw8ax0jZS01d23u/thSUslTayl/zWSlpT/\nfLmkF9z9w/Kg9YKk8Y0ZMAAAQGsUT+jqI2lXpe388rYazOwzkpIkranvewEAAE5m8YQui9LmMfpO\nlbTM3Y/W571mdoOZ5ZpZ7t69e+MYEgAAQOsST+jKl9S30naipN0x+k7Vp6cW436vuy9w91R3T+3R\no0ccQwIAAGhd4gld6yUlm1mSmZ2usmCVU72TmV0gqYukVyo1r5L0FTPrYmZdJH2lvA0AAOCUUue3\nF9291MxuVllYaiPp/7r7ZjO7V1Kuux8PYNdIWuruXum9H5rZT1UW3CTpXnf/sGk/AgAAQMtnlTJS\ni5Camuq5ubnNPQwAAIA6mdkGd0+Npy93pAcAAAiA0AUAABAAoQsAACAAQhcAAEAAhC4AAIAACF0A\nAAABELoAAAACIHQBAAAEQOgCAAAIgNAFAAAQAKELAAAgAEIXAABAAIQuAACAAAhdAAAAARC6AAAA\nAiB0AQAABEDoAgAACIDQBQAAEAChCwAAIABCFwAAQACELgAAgAAIXQAAAAEQugAAAAIgdAEAAARA\n6AIAAAiA0AUAABAAoQsAACAAQhcAAEAAhC4AAIAACF0AAAABELoAAAACIHQBAAAEQOgCAAAIgNAF\nAAAQAKELAAAgAEIXAABAAIQuAACAAAhdAAAAARC6AAAAAiB0AQAABEDoAgAACIDQBQAAEAChCwAA\nIABCFwAAQACELgAAgAAIXQAAAAEQugAAAAIgdAEAAARA6AIAAAiA0AUAABAAoQsAACAAQhcAAEAA\nhC4AAIAACF0AAAABELoAAAACIHQBAAAEQOgCAAAIgNAFAAAQAKELAAAgAEIXAABAAIQuAACAAAhd\nAAAAARC6AAAAAiB0AQAABEDoAgAACIDQBQAAEAChCwAAIABCFwAAQACELgAAgAAIXQAAAAEQugAA\nAAIgdAEAAAQQV+gys/Fm9paZbTWzO2L0mWxmW8xss5n9b6X2n5e3vWlmD5qZNdXgAQAAWouEujqY\nWRtJWZLGScqXtN7Mctx9S6U+yZJ+JOlSd99nZj3L2z8n6VJJKeVd/y7pS5L+3JQfAgAAoKWLZ6Vr\npKSt7r7d3Q9LWippYrU+35WU5e77JMnd95S3u6T2kk6X1E5SW0nvN8XAAQAAWpN4QlcfSbsqbeeX\nt1V2vqTzzexlM1trZuMlyd1fkfSSpMLy1yp3f7P6LzCzG8ws18xy9+7d25DPAQAA0KLFE7qiXYPl\n1bYTJCVLukzSNZIeNbPOZhaRNFBSosqC2hgz+2KNYu4L3D3V3VN79OhRn/EDAAC0CvGErnxJfStt\nJ0raHaVPtrsfcfcdkt5SWQi7WtJad//Y3T+W9LykSxo/bAAAgNYlntC1XlKymSWZ2emSpkrKqdZn\nuaTRkmRm3VV2unG7pHclfcnMEsysrcouoq9xehEAAOBkV2focvdSSTdLWqWywPR7d99sZveaWXp5\nt1WSisxsi8qu4brV3YskLZO0TdLrkv4p6Z/u/uwJ+BwAAAAtmrlXvzyreaWmpnpubm5zDwMAAKBO\nZrbB3VPj6csd6QEAAAIgdAEAAARA6AIAAAiA0AUAABAAoQsAACAAQhcAAEAAhC4AAIAACF0AAAAB\nELoAAAACIHQBAAAEQOgCAAAIgNAFAAAQAKELAAAgAEIXAABAAIQuAACAAAhdAAAAARC6AAAAAiB0\nAQAABEDoAgAACIDQBQAAEAChCwAAIABCFwAAQACELgAAgAAIXQAAAAEQugAAAAIgdAEAAARA6AIA\nAAiA0AUAABAAoQsAACAAQhcAAEAAhC4AAIAACF0AAAABELoAAAACIHQBAAAEQOgCAAAIgNAFAAAQ\nAKELAAAgAEIXAABAAIQuAACAAAhdAAAAARC6AAAAAiB0AQAABEDoAgAACIDQBQAAEAChCwAAIABC\nFwAAQACELgAAgAAIXQAAAAEQugAAAAIgdAEAAARA6AIAAAiA0AUAABAAoQsAACAAQhcAAEAAhC4A\nAIAACF0AAAABELoAAAACIHQBAAAEQOgCAAAIgNAFAAAQAKELAAAgAEIXAABAAIQuAACAAAhdAAAA\nARC6AAAAAiB0AQAABEDoAgAACIDQBQAAEAChCwAAIABCFwAAQACELgAAgAAIXQAAAAHEFbrMbLyZ\nvWVmW83sjhh9JpvZFjPbbGb/W6n9PDP7k5m9Wb6/X9MMHQAAoPVIqKuDmbWRlCVpnKR8SevNLMfd\nt1TqkyzpR5Iudfd9ZtazUonFkua4+wtm1kHSsSb9BAAAAK1APCtdIyVtdfft7n5Y0lJJE6v1+a6k\nLHffJ0nuvkeSzOxCSQnu/kJ5+8fu/p8mGz0AAEArEU/o6iNpV6Xt/PK2ys6XdL6ZvWxma81sfKX2\nYjP7g5ltNLNflK+cVWFmN5hZrpnl7t27tyGfAwAAoEWLJ3RZlDavtp0gKVnSZZKukfSomXUub/+C\npB9KGiGpv6TrahRzX+Duqe6e2qNHj7gHDwAA0FrEE7ryJfWttJ0oaXeUPtnufsTdd0h6S2UhLF/S\nxvJTk6WSlku6uPHDBgAAaF3iCV3rJSWbWZKZnS5pqqScan2WSxotSWbWXWWnFbeXv7eLmR1fvhoj\naYsAAABOMXWGrvIVqpslrZL0pqTfu/tmM7vXzNLLu62SVGRmWyS9JOlWdy9y96MqO7W42sxeV9mp\nyv85ER8EAACgJTP36pdnNa/U1FTPzc1t7mEAAADUycw2uHtqPH25Iz0AAEAAhC4AAIAACF0AAAAB\nELoAAAACIHQBAAAEQOgCAAAIgNAFAAAQAKELAAAgAEIXAABAAIQuAACAABKaewDNKeuGFyWzTxvc\nNXPBl5tvQAAA4KR1yq50VQSuaq+sG15s7qEBAICT0Km70nU8aFVvAwAAOAFO2ZUuAACAkAhdAAAA\nAZy6ocu97FVXGwAAQBM4ZUPXzAVf/jRkVXrx7UUAAHAinLoX0ksELAAAEMwpu9IFAAAQEqELAAAg\nAEIXAABAAIQuAACAAAhdAAAAARC6AAAAAiB0AQAABEDoAgAACIDQBQAAEAChCwAAIABCFwAAQACE\nLgAAgAAIXQAAAAEQugAAAAIgdAEAAARA6AIAAAiA0AUAABAAoQsAACAAQhcAAEAAhC4AAIAACF0A\nAAABELoAAAACIHQBAAAEQOgCAAAIgNAFAAAQAKELAAAgAEIXAABAAIQuAACAAAhdAAAAARC6AAAA\nAiB0AQAABEDoAgAACIDQBQAAEAChCwAAIABCFwAAQACELgAAgAAIXQAAAAEQugAAAAIgdAEAAARA\n6AIAAAiA0AUAABBAQnMPAAAAoLGybnhRMvu0wV0zF3y5+QYUBStdAACgVasIXNVeWTe82NxDq4KV\nLgAA0LodD1rV21oYVroAAAACIHQBAAAEQOgCAACtm3vZq662ZkboAgAArdrMBV/+NGRVerW0by9y\nIT0AAGj1WlrAioaVLgAAgAAIXQAAAAEQugAAAAIgdAEAAARA6AIAAAggrtBlZuPN7C0z22pmd8To\nM9nMtpjZZjP732r7zjazAjN7qCkGDQAA0NrUecsIM2sjKUvSOEn5ktabWY67b6nUJ1nSjyRd6u77\nzKxntTI/lfSXphs2AABA6xLPStdISVvdfbu7H5a0VNLEan2+KynL3fdJkrvvOb7DzIZLOkfSn5pm\nyAAAAK1PPKGrj6Rdlbbzy9sqO1/S+Wb2spmtNbPxkmRmp0n6laRba/sFZnaDmeWaWe7evXvjHz0A\nAEArEU/osiht1R9mlCApWdJlkq6R9KiZdZZ0k6QV7r5LtXD3Be6e6u6pPXr0iGNIAAAArUs8jwHK\nl9S30naipN1R+qx19yOSdpjZWyoLYaMkfcHMbpLUQdLpZvaxu0e9GB8AAOBkFc9K13pJyWaWZGan\nS5oqKadan+WSRkuSmXVX2enG7e7+LXc/z937SfqhpMUELgAAcCqqM3S5e6mkmyWtkvSmpN+7+2Yz\nu9fM0su7rZJUZGZbJL0k6VZ3LzpRgwYAAGhtzL365VnNKzU11XNzc5t7GAAAAHUysw3unhpPX+5I\nDwAAEAChCwAAIABCFwAAQACELgAAgAAIXQAAAAEQugAAAAIgdAEAAARA6AIAAAiA0AUAABAAoQsA\nACAAQhcAAEAAhC4AAIAACF0AAAABELoAAAACIHQBAAAEQOgCAAAIgNAFAAAQAKELAAAgAEIXAABA\nAIQuAACAAAhdAAAAARC6AAAAAiB0AQAABEDoAgAACIDQBQAAEAChCwAAIABCFwAAQACELgAAgADM\n3Zt7DFWY2V5J75yA0t0lfXAC6p5qmMfGYw4bjzlsGsxj4zGHjdfa5/Az7t4jno4tLnSdKGaW6+6p\nzT2O1o55bDzmsPGYw6bBPDYec9h4p9IccnoRAAAgAEIXAABAAKdS6FrQ3AM4STCPjcccNh5z2DSY\nx8ZjDhvvlJnDU+aaLgAAgOZ0Kq10AQAANJuTLnSZ2f81sz1m9kaM/WZmD5rZVjP7l5ldHHqMLV0c\nc3iZme03s03lr7tDj7GlM7O+ZvaSmb1pZpvN7PtR+nAs1iLOOeRYrIOZtTezdWb2z/J5/P+i9Gln\nZk+WH4uvmlm/8CNtueKcw+vMbG+lY/H65hhrS2dmbcxso5n9Mcq+k/44TGjuAZwAj0t6SNLiGPsn\nSEouf6VJml/+Jz71uGqfQ0n6m7tfFWY4rVKppP/H3V8zs46SNpjZC+6+pVIfjsXaxTOHEsdiXQ5J\nGuPuH5tZW0l/N7Pn3X1tpT7TJe1z94iZTZU0T9KU5hhsCxXPHErSk+5+czOMrzX5vqQ3JZ0dZd9J\nfxyedCtd7v5XSR/W0mWipMVeZq2kzmbWK8zoWoc45hB1cPdCd3+t/OePVPaXTJ9q3TgWaxHnHKIO\n5cfXx+Wbbctf1S/mnShpUfnPyySNNTMLNMQWL845RB3MLFHSlZIejdHlpD8OT7rQFYc+knZV2s4X\nf5E3xKjypfbnzWxQcw+mJStfIh8m6dVquzgW41TLHEoci3UqP6WzSdIeSS+4e8xj0d1LJe2X1C3s\nKFu2OOZQkr5RfqnAMjPrG3iIrcFvJN0m6ViM/Sf9cXgqhq5oqZn/Y6mf11T22IMhkv6PpOXNPJ4W\ny8w6SHpa0g/c/UD13VHewrFYTR1zyLEYB3c/6u5DJSVKGmlmF1XrwrFYhzjm8FlJ/dw9RdKL+nTF\nBpLM7CpJe9x9Q23dorSdVMfhqRi68iVV/j+QREm7m2ksrZK7Hzi+1O7uKyS1NbPuzTysFqf82o+n\nJf3O3f8QpQvHYh3qmkOOxfpx92JJf5Y0vtquimPRzBIkdRKXGEQVaw7dvcjdD5Vv/o+k4YGH1tJd\nKindzHZKWippjJn9tlqfk/44PBVDV46kaeXfHLtE0n53L2zuQbUmZnbu8fPsZjZSZcdRUfOOqmUp\nn5+Fkt509/tjdONYrEU8c8ixWDcz62Fmnct/PkPSlyX9u1q3HEmZ5T9nSFrj3MSxQjxzWO16zHSV\nXYOIcu7+I3dPdPd+kqaq7Bj7r2rdTvrj8KT79qKZLZF0maTuZpYv6R6VXfQod39E0gpJV0jaKuk/\nkr7dPCNtueKYwwxJN5pZqaSDkqaebP9hNIFLJV0r6fXy60Ak6f+VdJ7EsRineOaQY7FuvSQtMrM2\nKgulv3f3P5rZvZJy3T1HZeH2CTPbqrKVhanNN9wWKZ45nGVm6Sr71u2Hkq5rttG2Iqfaccgd6QEA\nAAI4FU8vAgAABEfoAgAACIDQBQAAEAChCwAAIABCFwAAQACELgAAgAAIXQAAAAEQugAAAAL4/wF6\nMR0f1jWG6AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2ad9937c0b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Analysis\n",
    "\n",
    "plt.figure(1, figsize=(10, 10))\n",
    "\n",
    "for seq in unique_seqs:\n",
    "    df_current = df_seqs_vectors[df_seqs_vectors['label']==seq]\n",
    "    plt.scatter(df_current['x'].values, df_current['y'].values, color=next(palette), label=seq)\n",
    "\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
