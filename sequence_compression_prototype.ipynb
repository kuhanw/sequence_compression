{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import itertools\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createLSTMLayers(n_layers, n_cells, dropout):\n",
    "    cell_list = []\n",
    "    \n",
    "    for layer in range(n_layers):\n",
    "        cell = tf.contrib.rnn.LayerNormBasicLSTMCell(n_cells, activation=tf.nn.relu, layer_norm=False)\n",
    "        \n",
    "        cell_list.append(cell)\n",
    "        \n",
    "    return cell_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padSeq(seqs, pad_length, right_pad=True):\n",
    "    \n",
    "    '''\n",
    "    padSeq\n",
    "    \n",
    "    Pad sequences to be equal length.\n",
    "    \n",
    "    seqs: sequences to be padded\n",
    "    pad_length: length to pad sequences\n",
    "    left_pad: Pad at the end of the sequences, or reverse pad from front\n",
    "    '''\n",
    "    \n",
    "    padded_seqs = np.zeros((len(seqs), pad_length))\n",
    "    \n",
    "    for idx_row, row in enumerate(seqs):\n",
    "        for idx_col, col in enumerate(row):\n",
    "            if right_pad:\n",
    "                padded_seqs[idx_row, idx_col] = seqs[idx_row][idx_col]\n",
    "            else:\n",
    "                padded_seqs[idx_row, pad_length-idx_col-1] = seqs[idx_row][len(row)-idx_col-1]\n",
    "    return padded_seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Network parameters\n",
    "encoder_layers = 1\n",
    "encoder_cells = 5\n",
    "encoder_dropout = 1\n",
    "\n",
    "decoder_layers = 1\n",
    "decoder_cells = 5\n",
    "decoder_dropout = 1\n",
    "\n",
    "seq_length = 5\n",
    "n_features = 1\n",
    "\n",
    "latent_dimensions = 2\n",
    "\n",
    "#Training Parameters\n",
    "lr = 0.001\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.placeholder(name='input', shape=[None, seq_length, n_features], dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    q = sess.run([encoder_final_state], {inputs:[[[2.], [2.],[2.], [2.], [2.]]]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoder\n",
    "with tf.variable_scope('encoder', reuse=False):\n",
    "    \n",
    "    encoder_cell_fw = tf.contrib.rnn.MultiRNNCell(createLSTMLayers(encoder_layers, encoder_cells, encoder_dropout))\n",
    "    encoder_cell_bw = tf.contrib.rnn.MultiRNNCell(createLSTMLayers(encoder_layers, encoder_cells, encoder_dropout))\n",
    "    \n",
    "    (encoder_fw_outputs, encoder_bw_outputs), encoder_state_outputs = tf.nn.bidirectional_dynamic_rnn(\n",
    "                                        encoder_cell_fw,\n",
    "                                        encoder_cell_bw, \n",
    "                                        inputs=inputs,\n",
    "                                        dtype=tf.float32, time_major=False, swap_memory=True)\n",
    "    \n",
    "    encoder_final_state = tf.concat([state_tuple[0].h for state_tuple in encoder_state_outputs], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_vector_in = tf.contrib.layers.fully_connected(encoder_final_state, latent_dimensions, activation_fn=tf.nn.relu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_vector_out = tf.contrib.layers.fully_connected(latent_vector_in, decoder_cells, activation_fn=tf.nn.relu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_inputs = tf.zeros([tf.shape(inputs)[0], seq_length, n_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_initial_state = tuple([tf.nn.rnn_cell.LSTMStateTuple(c=latent_vector_out, h=latent_vector_out) for i in \n",
    "                              range(decoder_layers)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#decoder\n",
    "\n",
    "with tf.variable_scope('decoder', reuse=False):\n",
    "    \n",
    "    decoder_cell_fw = tf.contrib.rnn.MultiRNNCell(createLSTMLayers(decoder_layers, decoder_cells, decoder_dropout))\n",
    "    decoder_cell_bw = tf.contrib.rnn.MultiRNNCell(createLSTMLayers(decoder_layers, decoder_cells, decoder_dropout))\n",
    "    \n",
    "    (decoder_fw_outputs, decoder_bw_outputs), decoder_state_outputs = tf.nn.bidirectional_dynamic_rnn(\n",
    "                                        decoder_cell_fw,\n",
    "                                        decoder_cell_bw, \n",
    "                                        inputs=decoder_inputs,\n",
    "                                        initial_state_fw=decoder_initial_state,\n",
    "                                        initial_state_bw=decoder_initial_state,\n",
    "                                        dtype=tf.float32, time_major=False, swap_memory=True)\n",
    "    \n",
    "    decoder_outputs = tf.concat([decoder_fw_outputs, decoder_bw_outputs], axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_layer = tf.contrib.layers.fully_connected(decoder_outputs, n_features, activation_fn=tf.nn.relu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.reduce_sum(tf.square(inputs-output_layer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(learning_rate=lr).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    q = sess.run([output_layer], {inputs:[[[2.], [2.], [2.], [2.], [2.]]]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_seqs = pd.DataFrame(padSeq(generatePseudoSequences(5, 1, 5, 50000, variable_length=True), 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_cols = df_seqs.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_seqs['label'] = df_seqs[seq_cols].apply(lambda x:'_'.join([str(i) for i in x]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = model_selection.train_test_split(df_seqs, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training\n",
    "\n",
    "n_epochs = 50\n",
    "\n",
    "n_batches = df_train.shape[0]//batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 250\n"
     ]
    }
   ],
   "source": [
    "print (n_epochs, n_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generatePseudoSequences(seq_length, low, high, n_seqs, seed, variable_length=False):\n",
    "    \n",
    "    '''\n",
    "    generatePseudoSequences\n",
    "    \n",
    "    seq_length: Length of sequence to generate\n",
    "    low: lower bound of values\n",
    "    high: upper bound of values\n",
    "    n_seqs: number of sequences to generate\n",
    "    variable_length: Create sequence of variable length    \n",
    "    '''\n",
    "    random.seed(seed)\n",
    "    #Generate random sequences\n",
    "    random_seqs = []\n",
    "    for seq in range(n_seqs):\n",
    "        if variable_length: current_length = random.randint(1, seq_length)\n",
    "        else: current_length = seq_length\n",
    "        current_seq = [random.randint(low, high) for _ in range(current_length)]\n",
    "        \n",
    "        random_seqs.append(current_seq)\n",
    "        \n",
    "    #Generate repeating sequences\n",
    "    \n",
    "    repeat_seqs = []\n",
    "    for seq in range(n_seqs):\n",
    "        if variable_length: current_length = random.randint(1, seq_length)\n",
    "        else: current_length = seq_length\n",
    "        seq_value = random.randint(low, high)\n",
    "        current_seq = [seq_value for _ in range(current_length)]\n",
    "        \n",
    "        repeat_seqs.append(current_seq)\n",
    "        \n",
    "    return random_seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin training\n",
      "epoch:0, batch:0, loss:1.15e+03\n",
      "epoch:0, batch:100, loss:352\n",
      "epoch:0, batch:200, loss:290\n",
      "epoch:1, batch:0, loss:249\n",
      "epoch:1, batch:100, loss:258\n",
      "epoch:1, batch:200, loss:257\n",
      "epoch:2, batch:0, loss:182\n",
      "epoch:2, batch:100, loss:153\n",
      "epoch:2, batch:200, loss:160\n",
      "epoch:3, batch:0, loss:121\n",
      "epoch:3, batch:100, loss:158\n",
      "epoch:3, batch:200, loss:145\n",
      "epoch:4, batch:0, loss:175\n",
      "epoch:4, batch:100, loss:167\n",
      "epoch:4, batch:200, loss:157\n",
      "epoch:5, batch:0, loss:127\n",
      "epoch:5, batch:100, loss:165\n",
      "epoch:5, batch:200, loss:143\n",
      "epoch:6, batch:0, loss:104\n",
      "epoch:6, batch:100, loss:180\n",
      "epoch:6, batch:200, loss:132\n",
      "epoch:7, batch:0, loss:147\n",
      "epoch:7, batch:100, loss:110\n",
      "epoch:7, batch:200, loss:163\n",
      "epoch:8, batch:0, loss:133\n",
      "epoch:8, batch:100, loss:121\n",
      "epoch:8, batch:200, loss:165\n",
      "epoch:9, batch:0, loss:162\n",
      "epoch:9, batch:100, loss:148\n",
      "epoch:9, batch:200, loss:123\n",
      "epoch:10, batch:0, loss:133\n",
      "epoch:10, batch:100, loss:119\n",
      "epoch:10, batch:200, loss:148\n",
      "epoch:11, batch:0, loss:121\n",
      "epoch:11, batch:100, loss:143\n",
      "epoch:11, batch:200, loss:159\n",
      "epoch:12, batch:0, loss:186\n",
      "epoch:12, batch:100, loss:113\n",
      "epoch:12, batch:200, loss:128\n",
      "epoch:13, batch:0, loss:130\n",
      "epoch:13, batch:100, loss:190\n",
      "epoch:13, batch:200, loss:127\n",
      "epoch:14, batch:0, loss:188\n",
      "epoch:14, batch:100, loss:124\n",
      "epoch:14, batch:200, loss:169\n",
      "epoch:15, batch:0, loss:126\n",
      "epoch:15, batch:100, loss:154\n",
      "epoch:15, batch:200, loss:139\n",
      "epoch:16, batch:0, loss:106\n",
      "epoch:16, batch:100, loss:153\n",
      "epoch:16, batch:200, loss:174\n",
      "epoch:17, batch:0, loss:156\n",
      "epoch:17, batch:100, loss:159\n",
      "epoch:17, batch:200, loss:139\n",
      "epoch:18, batch:0, loss:115\n",
      "epoch:18, batch:100, loss:145\n",
      "epoch:18, batch:200, loss:113\n",
      "epoch:19, batch:0, loss:145\n",
      "epoch:19, batch:100, loss:135\n",
      "epoch:19, batch:200, loss:186\n",
      "epoch:20, batch:0, loss:154\n",
      "epoch:20, batch:100, loss:160\n",
      "epoch:20, batch:200, loss:141\n",
      "epoch:21, batch:0, loss:156\n",
      "epoch:21, batch:100, loss:134\n",
      "epoch:21, batch:200, loss:118\n",
      "epoch:22, batch:0, loss:128\n",
      "epoch:22, batch:100, loss:166\n",
      "epoch:22, batch:200, loss:159\n",
      "epoch:23, batch:0, loss:168\n",
      "epoch:23, batch:100, loss:112\n",
      "epoch:23, batch:200, loss:121\n",
      "epoch:24, batch:0, loss:144\n",
      "epoch:24, batch:100, loss:119\n",
      "epoch:24, batch:200, loss:142\n",
      "epoch:25, batch:0, loss:180\n",
      "epoch:25, batch:100, loss:122\n",
      "epoch:25, batch:200, loss:117\n",
      "epoch:26, batch:0, loss:181\n",
      "epoch:26, batch:100, loss:148\n",
      "epoch:26, batch:200, loss:121\n",
      "epoch:27, batch:0, loss:147\n",
      "epoch:27, batch:100, loss:155\n",
      "epoch:27, batch:200, loss:121\n",
      "epoch:28, batch:0, loss:63.7\n",
      "epoch:28, batch:100, loss:142\n",
      "epoch:28, batch:200, loss:155\n",
      "epoch:29, batch:0, loss:114\n",
      "epoch:29, batch:100, loss:154\n",
      "epoch:29, batch:200, loss:121\n",
      "epoch:30, batch:0, loss:101\n",
      "epoch:30, batch:100, loss:123\n",
      "epoch:30, batch:200, loss:149\n",
      "epoch:31, batch:0, loss:104\n",
      "epoch:31, batch:100, loss:91.9\n",
      "epoch:31, batch:200, loss:95.8\n",
      "epoch:32, batch:0, loss:121\n",
      "epoch:32, batch:100, loss:119\n",
      "epoch:32, batch:200, loss:152\n",
      "epoch:33, batch:0, loss:98.4\n",
      "epoch:33, batch:100, loss:109\n",
      "epoch:33, batch:200, loss:172\n",
      "epoch:34, batch:0, loss:92.5\n",
      "epoch:34, batch:100, loss:178\n",
      "epoch:34, batch:200, loss:111\n",
      "epoch:35, batch:0, loss:139\n",
      "epoch:35, batch:100, loss:109\n",
      "epoch:35, batch:200, loss:115\n",
      "epoch:36, batch:0, loss:124\n",
      "epoch:36, batch:100, loss:104\n",
      "epoch:36, batch:200, loss:92.3\n",
      "epoch:37, batch:0, loss:107\n",
      "epoch:37, batch:100, loss:90.8\n",
      "epoch:37, batch:200, loss:151\n",
      "epoch:38, batch:0, loss:119\n",
      "epoch:38, batch:100, loss:107\n",
      "epoch:38, batch:200, loss:139\n",
      "epoch:39, batch:0, loss:129\n",
      "epoch:39, batch:100, loss:116\n",
      "epoch:39, batch:200, loss:123\n",
      "epoch:40, batch:0, loss:85.7\n",
      "epoch:40, batch:100, loss:122\n",
      "epoch:40, batch:200, loss:124\n",
      "epoch:41, batch:0, loss:132\n",
      "epoch:41, batch:100, loss:110\n",
      "epoch:41, batch:200, loss:132\n",
      "epoch:42, batch:0, loss:197\n",
      "epoch:42, batch:100, loss:146\n",
      "epoch:42, batch:200, loss:101\n",
      "epoch:43, batch:0, loss:97\n",
      "epoch:43, batch:100, loss:115\n",
      "epoch:43, batch:200, loss:88.2\n",
      "epoch:44, batch:0, loss:108\n",
      "epoch:44, batch:100, loss:122\n",
      "epoch:44, batch:200, loss:168\n",
      "epoch:45, batch:0, loss:127\n",
      "epoch:45, batch:100, loss:141\n",
      "epoch:45, batch:200, loss:125\n",
      "epoch:46, batch:0, loss:169\n",
      "epoch:46, batch:100, loss:112\n",
      "epoch:46, batch:200, loss:137\n",
      "epoch:47, batch:0, loss:146\n",
      "epoch:47, batch:100, loss:153\n",
      "epoch:47, batch:200, loss:138\n",
      "epoch:48, batch:0, loss:99.4\n",
      "epoch:48, batch:100, loss:135\n",
      "epoch:48, batch:200, loss:146\n",
      "epoch:49, batch:0, loss:92.4\n",
      "epoch:49, batch:100, loss:154\n",
      "epoch:49, batch:200, loss:124\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    print ('Begin training')\n",
    "    \n",
    "    for c_epoch in range(n_epochs):\n",
    "        df_epoch_samples = df_train[seq_cols].sample(frac=1)\n",
    "        for c_batch in range(n_batches): \n",
    "            batch = df_epoch_samples[c_batch*batch_size:(c_batch+1)*batch_size].values\n",
    "            batch = batch.reshape(len(batch), seq_length, 1)\n",
    "            _, c_loss = sess.run([optimizer, loss], {inputs:batch})\n",
    "            \n",
    "            if c_batch%100==0:\n",
    "                print ('epoch:%d, batch:%d, loss:%.3g' % (c_epoch, c_batch, c_loss))\n",
    "    \n",
    "    latent_vectors = sess.run(latent_vector_in, {inputs:df_train[seq_cols].values.reshape(df_train.shape[0], seq_length, 1)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vectors = pd.DataFrame(latent_vectors, columns=['x', 'y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_seqs_vectors = df_seqs.merge(df_vectors, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "palette = itertools.cycle(sns.color_palette())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_seqs = df_seqs_vectors['label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Analysis\n",
    "\n",
    "plt.figure(1, figsize=(10, 10))\n",
    "\n",
    "for seq in unique_seqs:\n",
    "    df_current = df_seqs_vectors[df_seqs_vectors['label']==seq]\n",
    "    plt.scatter(df_current['x'].values, df_current['y'].values, color=next(palette), label=seq)\n",
    "\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
