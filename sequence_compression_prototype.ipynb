{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import itertools\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createLSTMLayers(n_layers, n_cells, dropout):\n",
    "    cell_list = []\n",
    "    \n",
    "    for layer in range(n_layers):\n",
    "        cell = tf.contrib.rnn.LayerNormBasicLSTMCell(n_cells, activation=tf.nn.relu, layer_norm=False)\n",
    "        \n",
    "        cell_list.append(cell)\n",
    "        \n",
    "    return cell_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padSeq(seqs, pad_length, right_pad=True):\n",
    "    \n",
    "    '''\n",
    "    padSeq\n",
    "    \n",
    "    Pad sequences to be equal length.\n",
    "    \n",
    "    seqs: sequences to be padded\n",
    "    pad_length: length to pad sequences\n",
    "    left_pad: Pad at the end of the sequences, or reverse pad from front\n",
    "    '''\n",
    "    \n",
    "    padded_seqs = np.zeros((len(seqs), pad_length))\n",
    "    \n",
    "    for idx_row, row in enumerate(seqs):\n",
    "        for idx_col, col in enumerate(row):\n",
    "            if right_pad:\n",
    "                padded_seqs[idx_row, idx_col] = seqs[idx_row][idx_col]\n",
    "            else:\n",
    "                padded_seqs[idx_row, pad_length-idx_col-1] = seqs[idx_row][len(row)-idx_col-1]\n",
    "    return padded_seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "#Network parameters\n",
    "encoder_layers = 1\n",
    "encoder_cells = 6\n",
    "encoder_dropout = 1\n",
    "\n",
    "decoder_layers = 1\n",
    "decoder_cells = 6\n",
    "decoder_dropout = 1\n",
    "\n",
    "seq_length = 5\n",
    "n_features = 1\n",
    "\n",
    "embedding_dimensions = 13\n",
    "vocabulary_size = 10\n",
    "latent_dimensions = 2\n",
    "\n",
    "#Training Parameters\n",
    "lr = 0.001\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.placeholder(name='input', shape=[None, seq_length], dtype=tf.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_lengths = tf.placeholder(name='input_lengths', shape=[None], dtype=tf.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = tf.get_variable('embeddings', [vocabulary_size, embedding_dimensions])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_inputs = tf.nn.embedding_lookup(embedding_matrix, inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoder\n",
    "with tf.variable_scope('encoder', reuse=False):\n",
    "    \n",
    "    encoder_cell_fw = tf.contrib.rnn.MultiRNNCell(createLSTMLayers(encoder_layers, encoder_cells, encoder_dropout))\n",
    "    encoder_cell_bw = tf.contrib.rnn.MultiRNNCell(createLSTMLayers(encoder_layers, encoder_cells, encoder_dropout))\n",
    "    \n",
    "    (encoder_fw_outputs, encoder_bw_outputs), encoder_state_outputs = tf.nn.bidirectional_dynamic_rnn(\n",
    "                                        encoder_cell_fw,\n",
    "                                        encoder_cell_bw, \n",
    "                                        inputs=embedding_inputs,\n",
    "                                        dtype=tf.float32, time_major=False, swap_memory=True)\n",
    "    \n",
    "    encoder_final_state = tf.concat([state_tuple[0].h for state_tuple in encoder_state_outputs], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = []\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())    \n",
    "\n",
    "    results = sess.run([encoder_final_state], {inputs:[[2, 2, 2, 2, 2], [2, 2, 2, 2, 1]]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_vector_in = tf.contrib.layers.fully_connected(encoder_final_state, latent_dimensions, \n",
    "                                                     activation_fn=tf.nn.relu,\n",
    "                                                     normalizer_fn=tf.contrib.layers.batch_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_vector_out = tf.contrib.layers.fully_connected(latent_vector_in, decoder_cells, \n",
    "                                                      activation_fn=tf.nn.relu,\n",
    "                                                      normalizer_fn=tf.contrib.layers.batch_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_inputs = tf.zeros([tf.shape(inputs)[0], seq_length, n_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_initial_state = tuple([tf.nn.rnn_cell.LSTMStateTuple(c=latent_vector_out, h=latent_vector_out) for i in \n",
    "                              range(decoder_layers)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "#decoder\n",
    "\n",
    "with tf.variable_scope('decoder', reuse=False):\n",
    "    \n",
    "    decoder_cell_fw = tf.contrib.rnn.MultiRNNCell(createLSTMLayers(decoder_layers, decoder_cells, decoder_dropout))\n",
    "    decoder_cell_bw = tf.contrib.rnn.MultiRNNCell(createLSTMLayers(decoder_layers, decoder_cells, decoder_dropout))\n",
    "    \n",
    "    (decoder_fw_outputs, decoder_bw_outputs), decoder_state_outputs = tf.nn.bidirectional_dynamic_rnn(\n",
    "                                        decoder_cell_fw,\n",
    "                                        decoder_cell_bw, \n",
    "                                        inputs=decoder_inputs,\n",
    "                                        initial_state_fw=decoder_initial_state,\n",
    "                                        initial_state_bw=decoder_initial_state,\n",
    "                                        dtype=tf.float32, time_major=False, swap_memory=True)\n",
    "    \n",
    "    decoder_outputs = tf.reduce_sum([decoder_fw_outputs, decoder_bw_outputs], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I think what is confusing you here is that the input has to one hot encoded if they categorical!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_layer = tf.contrib.layers.fully_connected(decoder_outputs, vocabulary_size, activation_fn=tf.nn.relu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_probs = tf.contrib.layers.softmax(output_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_pred = tf.argmax(output_layer, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor 'softmax/Reshape_1:0' shape=(?, 5, 10) dtype=float32>,\n",
       " <tf.Tensor 'ArgMax:0' shape=(?, 5) dtype=int64>,\n",
       " <tf.Tensor 'input:0' shape=(?, 5) dtype=int32>)"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_probs, output_pred, inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = []\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())    \n",
    "    \n",
    "    print ('Begin training')\n",
    "\n",
    "    results = sess.run([loss, inputs, output_layer, prediction], {inputs:[[2, 2, 2, 2, 2]]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "masks = tf.sequence_mask(lengths=input_lengths, maxlen=seq_length, dtype=tf.float32, name='masks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here the inputs can dense as opposed to one hot encoded\n",
    "#loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=inputs, logits=output_layer)\n",
    "\n",
    "#try using sequence loss instead\n",
    "\n",
    "loss = tf.contrib.seq2seq.sequence_loss(logits=output_layer, targets=inputs, weights=masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss = tf.reduce_sum(tf.square(inputs-output_layer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(learning_rate=lr).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generatePseudoSequences(seq_length, low, high, n_seqs, seed, variable_length=False):\n",
    "    \n",
    "    '''\n",
    "    generatePseudoSequences\n",
    "    \n",
    "    seq_length: Length of sequence to generate\n",
    "    low: lower bound of values\n",
    "    high: upper bound of values\n",
    "    n_seqs: number of sequences to generate\n",
    "    variable_length: Create sequence of variable length    \n",
    "    '''\n",
    "    random.seed(seed)\n",
    "    \n",
    "    #Generate random sequences\n",
    "    random_seqs = []\n",
    "    for seq in range(n_seqs):\n",
    "        if variable_length: current_length = random.randint(1, seq_length)\n",
    "        else: current_length = seq_length\n",
    "        current_seq = [random.randint(low, high) for _ in range(current_length)]\n",
    "        \n",
    "        random_seqs.append(current_seq)\n",
    "        \n",
    "    #Generate repeating sequences    \n",
    "    repeat_seqs = []\n",
    "    for seq in range(n_seqs):\n",
    "        if variable_length: current_length = random.randint(1, seq_length)\n",
    "        else: current_length = seq_length\n",
    "        seq_value = random.randint(low, high)\n",
    "        current_seq = [seq_value for _ in range(current_length)]\n",
    "        \n",
    "        repeat_seqs.append(current_seq)\n",
    "        \n",
    "    #Generate ascending sequences    \n",
    "    ascend_seqs = []\n",
    "    for seq in range(n_seqs):\n",
    "        if variable_length: current_length = random.randint(1, seq_length)\n",
    "        else: current_length = seq_length\n",
    "        seq_value = random.randint(low, high)\n",
    "        current_seq = [seq_value+i for i in range(0, current_length, 1)]\n",
    "        \n",
    "        ascend_seqs.append(current_seq)\n",
    "    \n",
    "    #Generate descending sequences\n",
    "    descend_seqs = []\n",
    "    for seq in range(n_seqs):\n",
    "        if variable_length: current_length = random.randint(1, seq_length)\n",
    "        else: current_length = seq_length\n",
    "        seq_value = random.randint(low+2, high)\n",
    "        current_seq = [seq_value-i for i in range(0, current_length, 1)]\n",
    "        \n",
    "        descend_seqs.append(current_seq)\n",
    "    \n",
    "    #Generate skip sequences\n",
    "    skip_seqs = []\n",
    "    for seq in range(n_seqs):\n",
    "        if variable_length: current_length = random.randint(2, seq_length)\n",
    "        else: current_length = seq_length\n",
    "        skip_value = random.randint(1, 5)\n",
    "        seq_value = random.randint(low, high)\n",
    "        seq_value = seq_value-skip_value\n",
    "        current_seq = [seq_value+i*skip_value for i in range(0, current_length, 1)]\n",
    "        \n",
    "        skip_seqs.append(current_seq)\n",
    "            \n",
    "    #return np.concatenate([descend_seqs, ascend_seqs])\n",
    "    #return skip_seqs\n",
    "    return repeat_seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudo_seqs = generatePseudoSequences(seq_length, 1, 9, 50000, 0, variable_length=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudo_seqs_lengths = [len(i) for i in pseudo_seqs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_seqs = pd.DataFrame(padSeq(pseudo_seqs, seq_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_cols = df_seqs.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_seqs['seq_length'] = pseudo_seqs_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_seqs['label'] = df_seqs[seq_cols].apply(lambda x:'_'.join([str(i) for i in x]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_seqs['label'] = df_seqs[1]-df_seqs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_seqs['label'] = df_seqs[0].apply(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_seqs = df_seqs.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>seq_length</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>45052</th>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11317</th>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16620</th>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43590</th>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20159</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0    1    2    3    4  seq_length label\n",
       "45052  6.0  6.0  6.0  6.0  6.0           5   6.0\n",
       "11317  7.0  7.0  7.0  7.0  7.0           5   7.0\n",
       "16620  7.0  7.0  0.0  0.0  0.0           2   7.0\n",
       "43590  8.0  8.0  8.0  8.0  8.0           5   8.0\n",
       "20159  2.0  2.0  2.0  2.0  2.0           5   2.0"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_seqs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = model_selection.train_test_split(df_seqs, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.reset_index(drop=True, inplace=True)\n",
    "df_test.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training\n",
    "\n",
    "n_epochs = 50\n",
    "n_batches = df_train.shape[0]//batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin training\n",
      "epoch:0, batch:0, batch loss:2.23, test loss:2.26, norm batch loss:0.0174, norm test loss:0.0177\n",
      "epoch:0, batch:300, batch loss:1.06, test loss:1.08, norm batch loss:0.00828, norm test loss:0.00841\n",
      "epoch:1, batch:0, batch loss:0.902, test loss:1.09, norm batch loss:0.00705, norm test loss:0.00852\n",
      "epoch:1, batch:300, batch loss:0.469, test loss:0.396, norm batch loss:0.00366, norm test loss:0.00309\n",
      "epoch:2, batch:0, batch loss:0.426, test loss:0.497, norm batch loss:0.00333, norm test loss:0.00388\n",
      "epoch:2, batch:300, batch loss:0.217, test loss:0.226, norm batch loss:0.0017, norm test loss:0.00176\n",
      "epoch:3, batch:0, batch loss:0.223, test loss:0.222, norm batch loss:0.00175, norm test loss:0.00173\n",
      "epoch:3, batch:300, batch loss:0.212, test loss:0.233, norm batch loss:0.00166, norm test loss:0.00182\n",
      "epoch:4, batch:0, batch loss:0.162, test loss:0.226, norm batch loss:0.00127, norm test loss:0.00177\n",
      "epoch:4, batch:300, batch loss:0.175, test loss:0.188, norm batch loss:0.00137, norm test loss:0.00147\n",
      "epoch:5, batch:0, batch loss:0.167, test loss:0.208, norm batch loss:0.0013, norm test loss:0.00162\n",
      "epoch:5, batch:300, batch loss:0.166, test loss:0.152, norm batch loss:0.0013, norm test loss:0.00118\n",
      "epoch:6, batch:0, batch loss:0.182, test loss:0.162, norm batch loss:0.00142, norm test loss:0.00127\n",
      "epoch:6, batch:300, batch loss:0.189, test loss:0.141, norm batch loss:0.00147, norm test loss:0.0011\n",
      "epoch:7, batch:0, batch loss:0.164, test loss:0.213, norm batch loss:0.00128, norm test loss:0.00166\n",
      "epoch:7, batch:300, batch loss:0.0309, test loss:0.0411, norm batch loss:0.000241, norm test loss:0.000321\n",
      "epoch:8, batch:0, batch loss:0.119, test loss:0.043, norm batch loss:0.000932, norm test loss:0.000336\n",
      "epoch:8, batch:300, batch loss:0.0287, test loss:0.0349, norm batch loss:0.000224, norm test loss:0.000273\n",
      "epoch:9, batch:0, batch loss:0.0429, test loss:0.0365, norm batch loss:0.000336, norm test loss:0.000285\n",
      "epoch:9, batch:300, batch loss:0.0115, test loss:0.011, norm batch loss:8.96e-05, norm test loss:8.6e-05\n",
      "epoch:10, batch:0, batch loss:0.0132, test loss:0.0234, norm batch loss:0.000103, norm test loss:0.000183\n",
      "epoch:10, batch:300, batch loss:0.0126, test loss:0.0144, norm batch loss:9.87e-05, norm test loss:0.000112\n",
      "epoch:11, batch:0, batch loss:0.0383, test loss:0.0131, norm batch loss:0.000299, norm test loss:0.000102\n",
      "epoch:11, batch:300, batch loss:0.0123, test loss:0.0274, norm batch loss:9.58e-05, norm test loss:0.000214\n",
      "epoch:12, batch:0, batch loss:0.0107, test loss:0.00758, norm batch loss:8.33e-05, norm test loss:5.92e-05\n",
      "epoch:12, batch:300, batch loss:0.0121, test loss:0.0403, norm batch loss:9.47e-05, norm test loss:0.000315\n",
      "epoch:13, batch:0, batch loss:0.00621, test loss:0.0059, norm batch loss:4.85e-05, norm test loss:4.61e-05\n",
      "epoch:13, batch:300, batch loss:0.00464, test loss:0.0041, norm batch loss:3.62e-05, norm test loss:3.2e-05\n",
      "epoch:14, batch:0, batch loss:0.00638, test loss:0.00443, norm batch loss:4.98e-05, norm test loss:3.46e-05\n",
      "epoch:14, batch:300, batch loss:0.0126, test loss:0.00435, norm batch loss:9.87e-05, norm test loss:3.4e-05\n",
      "epoch:15, batch:0, batch loss:0.00372, test loss:0.00524, norm batch loss:2.9e-05, norm test loss:4.09e-05\n",
      "epoch:15, batch:300, batch loss:0.00401, test loss:0.00277, norm batch loss:3.13e-05, norm test loss:2.17e-05\n",
      "epoch:16, batch:0, batch loss:0.00476, test loss:0.00563, norm batch loss:3.72e-05, norm test loss:4.4e-05\n",
      "epoch:16, batch:300, batch loss:0.00294, test loss:0.00599, norm batch loss:2.29e-05, norm test loss:4.68e-05\n",
      "epoch:17, batch:0, batch loss:0.00286, test loss:0.00309, norm batch loss:2.24e-05, norm test loss:2.42e-05\n",
      "epoch:17, batch:300, batch loss:0.00302, test loss:0.00979, norm batch loss:2.36e-05, norm test loss:7.65e-05\n",
      "epoch:18, batch:0, batch loss:0.00332, test loss:0.00561, norm batch loss:2.6e-05, norm test loss:4.38e-05\n",
      "epoch:18, batch:300, batch loss:0.0121, test loss:0.0061, norm batch loss:9.42e-05, norm test loss:4.77e-05\n",
      "epoch:19, batch:0, batch loss:0.0103, test loss:0.00498, norm batch loss:8.01e-05, norm test loss:3.89e-05\n",
      "epoch:19, batch:300, batch loss:0.00581, test loss:0.0028, norm batch loss:4.54e-05, norm test loss:2.19e-05\n",
      "epoch:20, batch:0, batch loss:0.00768, test loss:0.0022, norm batch loss:6e-05, norm test loss:1.72e-05\n",
      "epoch:20, batch:300, batch loss:0.0014, test loss:0.0027, norm batch loss:1.09e-05, norm test loss:2.11e-05\n",
      "epoch:21, batch:0, batch loss:0.00134, test loss:0.0012, norm batch loss:1.04e-05, norm test loss:9.41e-06\n",
      "epoch:21, batch:300, batch loss:0.00175, test loss:0.00154, norm batch loss:1.37e-05, norm test loss:1.2e-05\n",
      "epoch:22, batch:0, batch loss:0.00302, test loss:0.00252, norm batch loss:2.36e-05, norm test loss:1.96e-05\n",
      "epoch:22, batch:300, batch loss:0.00176, test loss:0.00268, norm batch loss:1.38e-05, norm test loss:2.09e-05\n",
      "epoch:23, batch:0, batch loss:0.00295, test loss:0.00198, norm batch loss:2.3e-05, norm test loss:1.55e-05\n",
      "epoch:23, batch:300, batch loss:0.00195, test loss:0.00272, norm batch loss:1.52e-05, norm test loss:2.12e-05\n",
      "epoch:24, batch:0, batch loss:0.00301, test loss:0.00277, norm batch loss:2.35e-05, norm test loss:2.16e-05\n",
      "epoch:24, batch:300, batch loss:0.00236, test loss:0.00234, norm batch loss:1.85e-05, norm test loss:1.83e-05\n",
      "epoch:25, batch:0, batch loss:0.00108, test loss:0.00233, norm batch loss:8.47e-06, norm test loss:1.82e-05\n",
      "epoch:25, batch:300, batch loss:0.00259, test loss:0.00124, norm batch loss:2.03e-05, norm test loss:9.68e-06\n",
      "epoch:26, batch:0, batch loss:0.00105, test loss:0.0124, norm batch loss:8.2e-06, norm test loss:9.67e-05\n",
      "epoch:26, batch:300, batch loss:0.00127, test loss:0.00424, norm batch loss:9.91e-06, norm test loss:3.31e-05\n",
      "epoch:27, batch:0, batch loss:0.00105, test loss:0.00309, norm batch loss:8.24e-06, norm test loss:2.41e-05\n",
      "epoch:27, batch:300, batch loss:0.000938, test loss:0.000866, norm batch loss:7.33e-06, norm test loss:6.77e-06\n",
      "epoch:28, batch:0, batch loss:0.0126, test loss:0.00129, norm batch loss:9.84e-05, norm test loss:1.01e-05\n",
      "epoch:28, batch:300, batch loss:0.00077, test loss:0.000678, norm batch loss:6.01e-06, norm test loss:5.3e-06\n",
      "epoch:29, batch:0, batch loss:0.000662, test loss:0.0016, norm batch loss:5.17e-06, norm test loss:1.25e-05\n",
      "epoch:29, batch:300, batch loss:0.0025, test loss:0.00176, norm batch loss:1.96e-05, norm test loss:1.38e-05\n",
      "epoch:30, batch:0, batch loss:0.0336, test loss:0.0283, norm batch loss:0.000263, norm test loss:0.000221\n",
      "epoch:30, batch:300, batch loss:0.00287, test loss:0.00133, norm batch loss:2.24e-05, norm test loss:1.04e-05\n",
      "epoch:31, batch:0, batch loss:0.00132, test loss:0.00136, norm batch loss:1.03e-05, norm test loss:1.06e-05\n",
      "epoch:31, batch:300, batch loss:0.00112, test loss:0.000943, norm batch loss:8.76e-06, norm test loss:7.37e-06\n",
      "epoch:32, batch:0, batch loss:0.00279, test loss:0.00179, norm batch loss:2.18e-05, norm test loss:1.4e-05\n",
      "epoch:32, batch:300, batch loss:0.00186, test loss:0.00124, norm batch loss:1.45e-05, norm test loss:9.69e-06\n",
      "epoch:33, batch:0, batch loss:0.012, test loss:0.00131, norm batch loss:9.36e-05, norm test loss:1.02e-05\n",
      "epoch:33, batch:300, batch loss:0.000587, test loss:0.000818, norm batch loss:4.59e-06, norm test loss:6.39e-06\n",
      "epoch:34, batch:0, batch loss:0.000606, test loss:0.0148, norm batch loss:4.74e-06, norm test loss:0.000116\n",
      "epoch:34, batch:300, batch loss:0.00158, test loss:0.00122, norm batch loss:1.24e-05, norm test loss:9.5e-06\n",
      "epoch:35, batch:0, batch loss:0.000688, test loss:0.00109, norm batch loss:5.37e-06, norm test loss:8.49e-06\n",
      "epoch:35, batch:300, batch loss:0.000564, test loss:0.000702, norm batch loss:4.41e-06, norm test loss:5.48e-06\n",
      "epoch:36, batch:0, batch loss:0.00138, test loss:0.000775, norm batch loss:1.08e-05, norm test loss:6.05e-06\n",
      "epoch:36, batch:300, batch loss:0.00129, test loss:0.00093, norm batch loss:1e-05, norm test loss:7.26e-06\n",
      "epoch:37, batch:0, batch loss:0.00301, test loss:0.000797, norm batch loss:2.35e-05, norm test loss:6.22e-06\n",
      "epoch:37, batch:300, batch loss:0.000402, test loss:0.000456, norm batch loss:3.14e-06, norm test loss:3.56e-06\n",
      "epoch:38, batch:0, batch loss:0.00087, test loss:0.000924, norm batch loss:6.79e-06, norm test loss:7.22e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:38, batch:300, batch loss:0.00209, test loss:0.000752, norm batch loss:1.63e-05, norm test loss:5.87e-06\n",
      "epoch:39, batch:0, batch loss:0.000451, test loss:0.00128, norm batch loss:3.53e-06, norm test loss:1e-05\n",
      "epoch:39, batch:300, batch loss:0.000415, test loss:0.000733, norm batch loss:3.24e-06, norm test loss:5.73e-06\n",
      "epoch:40, batch:0, batch loss:0.00051, test loss:0.00061, norm batch loss:3.99e-06, norm test loss:4.77e-06\n",
      "epoch:40, batch:300, batch loss:0.000443, test loss:0.00148, norm batch loss:3.46e-06, norm test loss:1.16e-05\n",
      "epoch:41, batch:0, batch loss:0.0005, test loss:0.00326, norm batch loss:3.91e-06, norm test loss:2.55e-05\n",
      "epoch:41, batch:300, batch loss:0.000798, test loss:0.00171, norm batch loss:6.23e-06, norm test loss:1.34e-05\n",
      "epoch:42, batch:0, batch loss:0.00353, test loss:0.00101, norm batch loss:2.76e-05, norm test loss:7.91e-06\n",
      "epoch:42, batch:300, batch loss:0.000908, test loss:0.000677, norm batch loss:7.09e-06, norm test loss:5.29e-06\n",
      "epoch:43, batch:0, batch loss:0.000513, test loss:0.00546, norm batch loss:4e-06, norm test loss:4.27e-05\n",
      "epoch:43, batch:300, batch loss:0.000202, test loss:0.000424, norm batch loss:1.58e-06, norm test loss:3.31e-06\n",
      "epoch:44, batch:0, batch loss:0.000557, test loss:0.000551, norm batch loss:4.35e-06, norm test loss:4.3e-06\n",
      "epoch:44, batch:300, batch loss:0.000222, test loss:0.000338, norm batch loss:1.74e-06, norm test loss:2.64e-06\n",
      "epoch:45, batch:0, batch loss:0.000255, test loss:0.000326, norm batch loss:1.99e-06, norm test loss:2.55e-06\n",
      "epoch:45, batch:300, batch loss:0.00169, test loss:0.000987, norm batch loss:1.32e-05, norm test loss:7.71e-06\n",
      "epoch:46, batch:0, batch loss:0.00105, test loss:0.000776, norm batch loss:8.22e-06, norm test loss:6.06e-06\n",
      "epoch:46, batch:300, batch loss:0.000385, test loss:0.000553, norm batch loss:3.01e-06, norm test loss:4.32e-06\n",
      "epoch:47, batch:0, batch loss:0.000386, test loss:0.000497, norm batch loss:3.02e-06, norm test loss:3.89e-06\n",
      "epoch:47, batch:300, batch loss:0.000439, test loss:0.000591, norm batch loss:3.43e-06, norm test loss:4.62e-06\n",
      "epoch:48, batch:0, batch loss:0.000418, test loss:0.000384, norm batch loss:3.27e-06, norm test loss:3e-06\n",
      "epoch:48, batch:300, batch loss:0.000525, test loss:0.00101, norm batch loss:4.1e-06, norm test loss:7.91e-06\n",
      "epoch:49, batch:0, batch loss:0.00274, test loss:0.00031, norm batch loss:2.14e-05, norm test loss:2.42e-06\n",
      "epoch:49, batch:300, batch loss:0.00348, test loss:0.000333, norm batch loss:2.72e-05, norm test loss:2.6e-06\n"
     ]
    }
   ],
   "source": [
    "metrics = []\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    print ('Begin training')\n",
    "    \n",
    "    for c_epoch in range(n_epochs):\n",
    "        df_train_current = df_train.sample(frac=1)\n",
    "        \n",
    "        for c_batch in range(n_batches): \n",
    "            batch = df_train_current[seq_cols][c_batch*batch_size:(c_batch+1)*batch_size].values\n",
    "            batch_lengths = df_train_current['seq_length'][c_batch*batch_size:(c_batch+1)*batch_size].values\n",
    "\n",
    "            #batch = batch.reshape(len(batch), seq_length, 1)\n",
    "            results = sess.run([optimizer, loss, encoder_final_state, latent_vector_in, latent_vector_out, output_layer], \n",
    "                               {inputs:batch, input_lengths:batch_lengths})\n",
    "            \n",
    "            \n",
    "            if c_batch%300==0:\n",
    "                df_test_current = df_test.sample(n=batch_size)\n",
    "                test_batch = df_test_current[seq_cols]\n",
    "                test_batch_lengths = df_test_current['seq_length']\n",
    "                #test_batch = test_batch.values.reshape(df_test_batch.shape[0], seq_length, 1)\n",
    "                results_test = sess.run([loss, output_layer], \n",
    "                               {inputs:test_batch, input_lengths:test_batch_lengths})            \n",
    "             \n",
    "                \n",
    "                print ('epoch:%d, batch:%d, batch loss:%.3g, test loss:%.3g, norm batch loss:%.3g, norm test loss:%.3g' % \n",
    "                       (c_epoch, c_batch, results[1], results_test[0], \n",
    "                        results[1]/batch_size, results_test[0]/batch_size))\n",
    "                \n",
    "                metrics.append([c_epoch, c_batch, results[1], results_test[0]])\n",
    "    \n",
    "        if c_epoch%10==0:\n",
    "            \n",
    "            latent_vectors_train = sess.run(latent_vector_in, \n",
    "                                      {inputs:df_train[seq_cols].values#.reshape(df_train.shape[0], seq_length, 1)\n",
    "                                      })\n",
    "            \n",
    "            latent_vectors_test = sess.run(latent_vector_in, \n",
    "                                      {inputs:df_test[seq_cols].values#.reshape(df_test.shape[0], seq_length, 1)\n",
    "                                      })\n",
    "            \n",
    "            total_output_test = sess.run(output_layer, \n",
    "                                         {inputs:df_test[seq_cols].values#.reshape(df_test.shape[0], seq_length, 1)\n",
    "                                         })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = np.array(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-265-42044337ca2f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m14\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msemilogy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'training loss'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msemilogy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'test loss'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'best'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\matplotlib\\pyplot.py\u001b[0m in \u001b[0;36msemilogy\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   3397\u001b[0m                       mplDeprecation)\n\u001b[0;32m   3398\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3399\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msemilogy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3400\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3401\u001b[0m         \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_hold\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwashold\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\matplotlib\\axes\\_axes.py\u001b[0m in \u001b[0;36msemilogy\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1674\u001b[0m         \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_hold\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1675\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_hold\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m  \u001b[1;31m# we've already processed the hold\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1676\u001b[1;33m         \u001b[0ml\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1677\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_hold\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mb\u001b[0m  \u001b[1;31m# restore the hold\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1678\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\matplotlib\\__init__.py\u001b[0m in \u001b[0;36minner\u001b[1;34m(ax, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1708\u001b[0m                     warnings.warn(msg % (label_namer, func.__name__),\n\u001b[0;32m   1709\u001b[0m                                   RuntimeWarning, stacklevel=2)\n\u001b[1;32m-> 1710\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1711\u001b[0m         \u001b[0mpre_doc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minner\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1712\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mpre_doc\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\matplotlib\\axes\\_axes.py\u001b[0m in \u001b[0;36mplot\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1436\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1437\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1438\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1439\u001b[0m             \u001b[0mlines\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1440\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\matplotlib\\axes\\_base.py\u001b[0m in \u001b[0;36madd_line\u001b[1;34m(self, line)\u001b[0m\n\u001b[0;32m   1757\u001b[0m             \u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_clip_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1758\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1759\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_line_limits\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1760\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_label\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1761\u001b[0m             \u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_label\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'_line%d'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlines\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\matplotlib\\axes\\_base.py\u001b[0m in \u001b[0;36m_update_line_limits\u001b[1;34m(self, line)\u001b[0m\n\u001b[0;32m   1779\u001b[0m         \u001b[0mFigures\u001b[0m \u001b[0mout\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdata\u001b[0m \u001b[0mlimit\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mgiven\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mupdating\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataLim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1780\u001b[0m         \"\"\"\n\u001b[1;32m-> 1781\u001b[1;33m         \u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1782\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvertices\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1783\u001b[0m             \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\matplotlib\\lines.py\u001b[0m in \u001b[0;36mget_path\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    949\u001b[0m         \"\"\"\n\u001b[0;32m    950\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_invalidy\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_invalidx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 951\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    952\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_path\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    953\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\matplotlib\\lines.py\u001b[0m in \u001b[0;36mrecache\u001b[1;34m(self, always)\u001b[0m\n\u001b[0;32m    655\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0malways\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_invalidy\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    656\u001b[0m             \u001b[0myconv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_yunits\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_yorig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 657\u001b[1;33m             \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_to_unmasked_float_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myconv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    658\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    659\u001b[0m             \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_y\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\matplotlib\\cbook\\__init__.py\u001b[0m in \u001b[0;36m_to_unmasked_float_array\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m   2006\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilled\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2007\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2008\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2009\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2010\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\euix\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\numpy\\core\\numeric.py\u001b[0m in \u001b[0;36masarray\u001b[1;34m(a, dtype, order)\u001b[0m\n\u001b[0;32m    529\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    530\u001b[0m     \"\"\"\n\u001b[1;32m--> 531\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    532\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    533\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "plt.figure(figsize=(14, 10))\n",
    "plt.semilogy([i for i in range(len(metrics[:,2]))], metrics[:,2], label='training loss')\n",
    "plt.semilogy([i for i in range(len(metrics[:,3]))], metrics[:,3], label='test loss')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vectors = pd.DataFrame(latent_vectors_train, columns=['x', 'y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Join to training data\n",
    "df_seqs_vectors = df_train.merge(df_vectors, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_seqs_vectors['label_add'] = df_seqs_vectors['label'].apply(str) + df_seqs_vectors[0].apply(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['3.06.0', '4.08.0', '4.07.0', '4.09.0', '2.04.0', '3.05.0',\n",
       "       '2.07.0', '3.09.0', '5.09.0', '1.09.0', '4.010.0', '2.010.0',\n",
       "       '5.06.0', '4.05.0', '1.04.0', '3.03.0', '1.010.0', '1.06.0',\n",
       "       '3.04.0', '4.03.0', '5.08.0', '1.05.0', '2.03.0', '5.05.0',\n",
       "       '5.04.0', '4.04.0', '2.05.0', '1.03.0', '2.08.0', '3.010.0',\n",
       "       '5.010.0', '5.03.0', '2.09.0', '2.06.0', '1.08.0', '1.07.0',\n",
       "       '3.08.0', '3.07.0', '4.06.0', '5.07.0'], dtype=object)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_seqs_vectors['label_add'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>label</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>label_add</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.723117</td>\n",
       "      <td>0.448238</td>\n",
       "      <td>3.03.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.358903</td>\n",
       "      <td>0.941783</td>\n",
       "      <td>4.03.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.486740</td>\n",
       "      <td>1.005442</td>\n",
       "      <td>2.03.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.049593</td>\n",
       "      <td>0.699132</td>\n",
       "      <td>1.03.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.358903</td>\n",
       "      <td>0.941783</td>\n",
       "      <td>4.03.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0    1     2     3    4  label         x         y label_add\n",
       "18  3.0  6.0   9.0  12.0  0.0    3.0  0.723117  0.448238    3.03.0\n",
       "27  3.0  7.0  11.0   0.0  0.0    4.0  0.358903  0.941783    4.03.0\n",
       "35  3.0  5.0   7.0   0.0  0.0    2.0  0.486740  1.005442    2.03.0\n",
       "41  3.0  4.0   5.0   6.0  7.0    1.0  1.049593  0.699132    1.03.0\n",
       "43  3.0  7.0  11.0   0.0  0.0    4.0  0.358903  0.941783    4.03.0"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_seqs_vectors[df_seqs_vectors[0]==3].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_col = 'label'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_seqs = df_seqs_vectors[label_col].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "palette = itertools.cycle(sns.color_palette(n_colors=len(unique_seqs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAl0AAAJCCAYAAAD6AnJlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x19383435f28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Analysis\n",
    "\n",
    "plt.figure(1, figsize=(10, 10))\n",
    "cmap = plt.cm.get_cmap('hsv', len(unique_seqs)+1)\n",
    "for idx_seq, seq in enumerate(unique_seqs):\n",
    "    df_current = df_seqs_vectors[df_seqs_vectors[label_col]==seq]\n",
    "    plt.scatter(df_current['x'].values, df_current['y'].values, color=cmap(idx_seq), label=seq)\n",
    "\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
